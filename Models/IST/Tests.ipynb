{"cells":[{"cell_type":"markdown","metadata":{"id":"cHy2H4YVh25D"},"source":["# Basic Setup and Functions"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"rXuB5FHah25G"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Using mps device\n"]}],"source":["%pip install snntorch --quiet\n","\n","import librosa, random\n","import numpy as np\n","import pandas as pd\n","import os\n","import soundfile as sf\n","\n","from pandas import DataFrame as df\n","import torch\n","\n","from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","import snntorch as snn\n","from snntorch.functional.acc import _population_code, _prediction_check\n","\n","import torch.nn as nn\n","from torch import Tensor\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch import optim\n","from torchvision import transforms\n","\n","from tqdm.notebook import tqdm\n","\n","import gc\n","\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"3ae_oxqGh25H"},"outputs":[],"source":["def Triangle_Network(num_inputs, num_outputs, beta=0.90, time_dependent = False):\n","    dy_dx = int(4/(num_outputs - num_inputs))\n","    hidden1 = num_inputs + (dy_dx * 1)\n","    hidden2 = num_inputs + (dy_dx * 2)\n","    hidden3 = num_inputs + (dy_dx * 3)\n","\n","    if beta and time_dependent:\n","        class Net(nn.Module):\n","        # Initialise network with 2 forward connections (linear connections) and 2 leaky integrated fire layers (hidden and output)\n","            def __init__(self, *args, **kwargs) -> None:\n","                super().__init__(*args, **kwargs)\n","                self.fc1 = nn.Linear(num_inputs, hidden1)\n","                self.lif1 = snn.Leaky(beta=beta)\n","                self.fc2 = nn.Linear(hidden1, hidden2)\n","                self.lif2 = snn.Leaky(beta=beta)\n","                self.fc3 = nn.Linear(hidden3, hidden3)\n","                self.lif3 = snn.Leaky(beta=beta)\n","                self.fc4 = nn.Linear(hidden3, num_outputs)\n","                self.lif4 = snn.Leaky(beta=beta)\n","\n","            # Define a forward pass assuming x is normalised data (i.e. all values in [0,1])\n","            def forward(self, x):\n","                mem1 = self.lif1.init_leaky()\n","                mem2 = self.lif2.init_leaky()\n","                mem3 = self.lif3.init_leaky()\n","                mem4 = self.lif4.init_leaky()\n","\n","                spk_rec = []\n","                mem_rec = []\n","\n","                # Insert data in shape (time x batch x features)\n","                for step in range(x.size(0)):\n","                    cur1 = self.fc1(x[step])\n","                    spk1, mem1 = self.lif1(cur1, mem1)\n","                    cur2 = self.fc2(spk1)\n","                    spk2, mem2 = self.lif2(cur2, mem2)\n","                    cur3 = self.fc3(spk2)\n","                    spk3, mem3 = self.lif3(cur3, mem3)\n","                    cur4 = self.fc4(spk3)\n","                    spk4, mem4 = self.lif4(cur4, mem4)\n","\n","                    spk_rec.append(spk4)\n","                    mem_rec.append(mem4)\n","\n","                return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n","            \n","        return Net()\n","\n","\n","    elif beta and not time_dependent: return nn.Sequential(nn.Flatten(),\n","                    nn.Linear(num_inputs, hidden1),\n","                    snn.Leaky(beta=beta, init_hidden=True),\n","                    nn.Linear(hidden1, hidden2),\n","                    snn.Leaky(beta=beta, init_hidden=True),\n","                    nn.Linear(hidden2, hidden3),\n","                    snn.Leaky(beta=beta, init_hidden=True),\n","                    nn.Linear(hidden3, num_outputs),\n","                    snn.Leaky(beta=beta, init_hidden=True, output=True))\n","\n","    else: return nn.Sequential(nn.Flatten(),\n","                    nn.Linear(num_inputs, hidden1),\n","                    nn.ReLU(),\n","                    nn.Linear(hidden1, hidden2),\n","                    nn.ReLU(),\n","                    nn.Linear(hidden2, hidden3),\n","                    nn.ReLU(),\n","                    nn.Linear(hidden3, num_outputs))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"_6kx2Evvh25I"},"outputs":[],"source":["def test_spiking_network(model, dataset, loss_fn, results: df, epoch, device, num_classes=False, printable=None, train_test = 'test'):\n","    dataloader = DataLoader(dataset, batch_size=100, num_workers=3, shuffle=False)\n","    model.eval()\n","    with torch.no_grad():\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        total_spikes = 0\n","        all_labels = []\n","        all_predicted = []\n","        all_probs = []\n","\n","        for data, labels in dataloader:\n","            x, labels = data.transpose(0, 1).to(device), labels.to(device)\n","            spikes, _ = model(x)\n","            test_loss += loss_fn(spikes, labels).item()\n","            \n","            if num_classes: _, predicted = _population_code(spikes, num_classes=num_classes, num_outputs=spikes.size(-1)).max(1)\n","            else: _, predicted = spikes.sum(dim=1).max(1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predicted.extend(predicted.cpu().numpy())\n","\n","            if num_classes: num_spikes = _population_code(spikes, num_classes=num_classes, num_outputs=spikes.size(-1))\n","            else: num_spikes = spikes.sum(dim=1)\n","            \n","            softmax = torch.nn.Softmax(dim=1)\n","            probabilities = softmax(num_spikes)\n","            all_probs.extend(probabilities.cpu().numpy())\n","        \n","            total_spikes += spikes.size(1)\n","\n","        test_loss /= total_spikes\n","\n","    # Accuracy\n","    accuracy = 100 * correct / total\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(all_labels, all_predicted)\n","\n","    # Recall/Sensitivity -- avoiding div by 0\n","    recall = recall_score(all_labels, all_predicted, average='weighted', zero_division=0) * 100\n","\n","    # Precision\n","    precision = precision_score(all_labels, all_predicted, average='weighted', zero_division=0) * 100\n","\n","    # F1 Score\n","    f1_score = (2 * precision * recall) / (precision + recall)\n","\n","    # AUC-ROC\n","    auc_roc = 100 * roc_auc_score(all_labels, all_probs, multi_class='ovr')\n","    \n","    if printable: printable.set_description(\n","        f'Epoch [{epoch + 1}] {train_test} Loss: {test_loss / len(dataloader):.2f} '\n","        f'{train_test} Accuracy: {accuracy:.2f}% F1: {f1_score}% Recall: {recall:.2f}% Precision: {precision:.2f}% '\n","        f'AUC-ROC: {auc_roc:.4f}%'\n","    )\n","\n","    results = results._append({\n","            'Epoch': epoch + 1,\n","            'Accuracy': accuracy,\n","            'F1': f1_score,\n","            'Recall': recall,\n","            'Precision': precision,\n","            'Test Loss': test_loss / len(dataloader),\n","            'AUC-ROC': auc_roc,\n","            'Confusion Matrix': cm\n","        }, ignore_index=True)\n","\n","    del data\n","    del labels\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    return results"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["class PopulationCrossEntropyLoss():\n","    def __init__(self, num_classes=2):\n","        self.num_classes = num_classes\n","        self.__name__ = \"PopulationCrossEntropyLoss\"\n","\n","    def __call__(self, spk_out, targets):\n","        loss_fn = nn.CrossEntropyLoss()\n","        \n","        _, _, num_outputs = _prediction_check(spk_out)\n","\n","        spike_count = _population_code(\n","                spk_out, self.num_classes, num_outputs\n","            )\n","\n","        loss = loss_fn(spike_count, targets)\n","\n","        return loss\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"q93J30hEh25J"},"outputs":[],"source":["LABEL_MAPPINGS = {\n","    'westernart/classical': 'Classical',\n","    'indierock/pop': 'Rock',\n","    'pop/soul/electronica': 'Electronic',\n","    'electronica': 'Electronic',\n","    'jazz': 'Jazz',\n","    'pop/hiphop/rock': 'Pop',\n","    'rap/hiphop': 'Hiphop',\n","    'rock': 'Rock',\n","    'rock/folk': 'Rock',\n","    'westernart/baroque': 'Classical',\n","    'electronica/dance': 'Electronic',\n","    'westernart/romantic': 'Classical',\n","    'blues': 'Jazz',\n","    'pop/folk': 'Pop',\n","    'westernart/romantic/classical': 'Classical',\n","    'pop/electronica': 'Electronic',\n","    'latin': 'Jazz',\n","    'country/folk': 'Country',\n","    'indierock/folk/pop': 'Rock',\n","    'jazz/blues': 'Jazz',\n","    'pop/rap/rock/hiphop': 'Pop',\n","    'pop/experimental': 'Pop',\n","    'blues/rock/jazz': 'Jazz',\n","    'jazz/adventure': 'Jazz',\n","    'blues/electronica': 'Jazz',\n","    'jazz/pop/soul': 'Jazz',\n","    'funk/electronica': 'Electronic',\n","    'folk/pop': 'Folk',\n","    'indierock/rock': 'Rock',\n","    'jazz/electronica': 'Electronic',\n","    'hiphop': 'Hiphop',\n","    'funk/rnb/adventure': 'Soul',\n","    'pop': 'Pop',\n","    'hiphop/rap': 'Hiphop',\n","    'pop/gospel': 'Soul',\n","    'rap/metal/electronica': 'Electronic',\n","    'pop/rock/folk': 'Rock',\n","    'pop/electronica/hiphop': 'Pop',\n","    'metal/rap': 'Hiphop',\n","    'country': 'Country',\n","    'rap/metal': 'Hiphop',\n","    'country/pop': 'Country',\n","    'folk': 'Folk',\n","    'pop/rock/dance': 'Pop',\n","    'dance': 'Electronic',\n","    'pop/jazz/latin': 'Jazz',\n","    'pop/jazz': 'Jazz',\n","    'funk/rnb/electronica': 'Electronic',\n","    'funk/blues/jazz': 'Jazz',\n","    'pop/rock/soul': 'Pop',\n","    'pop/hiphop': 'Pop',\n","    'blues/funk': 'Jazz',\n","    'rap/metal/hiphop': 'Hiphop',\n","    'blues/jazz/adventure': 'Jazz',\n","    'folk/indierock': 'Folk',\n","    'adventure': 'Classical',\n","    'metal/rock': 'Rock',\n","    'blues/rock/country': 'Jazz',\n","    'pop/soul/rnb': 'Soul',\n","    'blues/rock': 'Jazz',\n","    'blues/rock/indierock': 'Jazz',\n","    'country/pop/folk': 'Country',\n","    'country/blues/rock': 'Country',\n","    'rock/funk/country': 'Rock',\n","    'pop/rock': 'Rock',\n","    'pop/blues': 'Rock',\n","    'blues/indierock': 'Rock',\n","    'blues/rock/rnb': 'Rock',\n","    'blues/pop/folk': 'Jazz',\n","    'pop/funk/adventure': 'Pop',\n","    'blues/rock/pop': 'Rock',\n","    'folk/pop/funk': 'Folk'\n","}\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"y5-_se-YiO7P"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 'Classical', 1: 'Country', 2: 'Electronic', 3: 'Folk', 4: 'Hiphop', 5: 'Jazz', 6: 'Pop', 7: 'Rock', 8: 'Soul'}\n"]}],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    !rsync -av --exclude='mel_spectrograms' --exclude='cqt_spectrograms' /content/drive/MyDrive/spectrogram_tensors/ /content/spectrogram_tensors/ --quiet\n","    FILEPATH = '/content'\n","    CSV = 'sample_ISD.csv'\n","\n","    dataset = pd.read_csv(f'/content/drive/MyDrive/{CSV}', index_col=0)\n","\n","    \n","except:\n","    FILEPATH = \"../../Datasets/SmallDataset\"\n","    ORIGINAL_DIR = \"audio\"\n","    SAMPLE_DIR = \"audio uncompressed samples\"\n","    COMPRESSED_DIR = \"audio compressed\"\n","    CSV = 'sample_ISD.csv'\n","\n","    dataset = pd.read_csv(f'{FILEPATH}/{CSV}', index_col=0)\n","\n","X = dataset['filename'].tolist()\n","Y = dataset[dataset['supercategory']=='music']['category'].map(lambda x: LABEL_MAPPINGS[x]).tolist()\n","label_encoder = LabelEncoder()\n","Y_encoded = label_encoder.fit_transform(Y)\n","\n","label_mappings = {encoded_label: original_label for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n","print(label_mappings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Garbage collection special commands\n","\n","gc.collect()\n","if device == \"cuda\":\n","    torch.cuda.empty_cache()\n","    torch.cuda.memory_summary(device=None, abbreviated=False)\n","elif device == \"mps\":\n","    torch.mps.empty_cache()\n","    print(f\"MPS occupied memory: {torch.mps.driver_allocated_memory()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Very special command -- remove all variables\n","%reset"]},{"cell_type":"markdown","metadata":{"id":"JjzopoGZh25J"},"source":["# Audio Representation"]},{"cell_type":"markdown","metadata":{"id":"QrnOeGaHh25K"},"source":["## Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qroeHpJh25L"},"outputs":[],"source":["def sample(audio_path, duration=5.0, sr=44100):\n","    original_path = audio_path\n","    for ext in [\".m4a\", \".wav\", \".ogg\", \".flac\", \".mp3\"]:\n","        if os.path.exists(audio_path + ext):\n","            audio_path += ext\n","\n","            total_duration = librosa.get_duration(path=audio_path)\n","            y, _ = librosa.load(audio_path, sr=sr, duration=total_duration)\n","\n","            if total_duration < duration:\n","                pad_length = int((duration - total_duration) * sr)\n","                y = np.pad(y, (0, pad_length), mode='constant')\n","\n","            start = random.uniform(0, max(0, total_duration - duration))\n","            y = y[int(start * sr):int((start + duration) * sr)]\n","\n","            sf.write(f\"{FILEPATH}/{SAMPLE_DIR}/{original_path.split('/')[-1]}.wav\", y, sr)\n","\n","            return y"]},{"cell_type":"markdown","metadata":{"id":"43m8jiuhh25L"},"source":["## Bitrate and Compression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74ui8zpqh25M"},"outputs":[],"source":["import os\n","from tqdm.notebook import tqdm\n","from concurrent.futures import ThreadPoolExecutor\n","\n","def convert_to_mp3(input_file, output_file, sample_rate=16000, bit_rate=\"8k\", channels=1):\n","    !ffmpeg -i \"$input_file\" -ar \"$sample_rate\" -b:a \"$bitrate\" -ac \"$channels\" \"$output_file\" -hide_banner -loglevel error\n","\n","def convert_directory_to_mp3(input_dir, output_dir, sample_rate=16000, bit_rate=\"8k\"):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    audio_files = [file for file in os.listdir(input_dir) if file.endswith((\".m4a\", \".wav\", \".ogg\", \".flac\", \".mp3\"))]\n","\n","    for file in tqdm(audio_files, desc=\"Converting\"):\n","            input_file_path = os.path.join(input_dir, file)\n","            output_file_path = os.path.join(output_dir, os.path.splitext(file)[0] + \".mp3\")\n","            convert_to_mp3(input_file_path, output_file_path, sample_rate, bit_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAUf5u8Yh25M"},"outputs":[],"source":["bitdepths = np.array([2,4,8,16,24])\n","samplerates = np.int32(np.array([8,16,22.05,32,44.1])*1000)\n","\n","with ThreadPoolExecutor() as executor:\n","    for bitdepth in bitdepths:\n","        for samplerate in samplerates:\n","            bitrate = (bitdepth * samplerate) / 1000\n","            print(f\"bitdepth: {bitdepth}, samplerate: {samplerate}\")\n","            print(f\"effective bitrate: {bitrate} kbps\")\n","\n","            executor.submit(convert_directory_to_mp3(f\"{FILEPATH}/{SAMPLE_DIR}\", f\"{FILEPATH}/{COMPRESSED_DIR}/{bitdepth}-{samplerate}\", sample_rate=samplerate, bit_rate=f\"{bitrate}k\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEmydtY2h25M"},"outputs":[],"source":["# Assuming the directory contains all compressed files\n","!find . -mindepth 1 -maxdepth 1 -type d -exec sh -c 'find \"$1\" -type f -exec ls -l {} \\; | awk \"{sum += \\$5} END {print \\\"$1\\\", sum}\"' _ {} \\"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0K_lIJiLh25N"},"outputs":[],"source":["from mutagen.mp3 import MP3\n","\n","def get_unpacked_size(mp3_file):\n","    audio = MP3(mp3_file)\n","    duration = audio.info.length  # Duration of the audio in seconds\n","    bitrate = audio.info.bitrate  # Bitrate of the audio in bits per second\n","    # Calculate the unpacked size based on bitrate and duration\n","    unpacked_size = (duration * bitrate) / 8\n","    return unpacked_size\n","\n","\n","file_dirs = [d for d in os.listdir(f\"{FILEPATH}/{COMPRESSED_DIR}\") if os.path.isdir(f\"{FILEPATH}/{COMPRESSED_DIR}/{d}\")]\n","for bitrate in file_dirs:\n","    audio_files = [file for file in os.listdir(f\"{FILEPATH}/{COMPRESSED_DIR}/{bitrate}\") if file.endswith((\".mp3\"))]\n","    size = 0\n","    for file in audio_files:\n","        if os.path.exists(f\"{FILEPATH}/{COMPRESSED_DIR}/{bitrate}/{file}\"):\n","            size += get_unpacked_size(f\"{FILEPATH}/{COMPRESSED_DIR}/{bitrate}/{file}\")\n","    print(f\"{bitrate.split('/')[-1]}: {size} bytes\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BW6hyuwDh25N"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import tikzplotlib\n","\n","bit_two = np.array([[16,32,44.1,64,88.2],[1,3,4,5,5]]).T\n","bit_four = np.array([[32,64,88.2,128,176.4],[2,4,5,5,5]]).T\n","bit_eight = np.array([[64,128,176.4,256,352.8],[2,4,5,5,5]]).T\n","bit_sixteen = np.array([[128,256,352.8,512,705.6],[2,3,4,4,5]]).T\n","bit_twentyfour = np.array([[192,384,529.2,768,1058.4],[2,4,5,5,5]]).T\n","\n","plt.plot(bit_two[:,0], bit_two[:,1], label=\"2-bit\")\n","plt.plot(bit_four[:,0], bit_four[:,1], label=\"4-bit\")\n","plt.plot(bit_eight[:,0], bit_eight[:,1], label=\"8-bit\")\n","plt.plot(bit_sixteen[:,0], bit_sixteen[:,1], label=\"16-bit\")\n","plt.plot(bit_twentyfour[:,0], bit_twentyfour[:,1], label=\"24-bit\")\n","\n","plt.ylabel(\"Perceived Quality\")\n","plt.xlabel(\"Bitrate (kbps)\")\n","plt.xscale(\"log\")\n","plt.xlim(10,1100)\n","plt.ylim(0, 6)\n","plt.grid(True, which='both', axis='y')\n","#plt.legend()\n","\n","tikzplotlib.save(\"AudioRep/CompressionReception.tex\")"]},{"cell_type":"markdown","metadata":{"id":"st4_dGj4h25N"},"source":["## Spectrograms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yE7h8re8h25O"},"outputs":[],"source":["import spectrograms\n","\n","AUDIO_DIR = \"compressed_audio\"\n","\n","X = dataset[dataset['supercategory']=='music']['filename'].tolist()\n","Y = dataset[dataset['supercategory']=='music']['category'].map(lambda x: LABEL_MAPPINGS[x]).tolist()\n","\n","label_encoder = LabelEncoder()\n","Y_encoded = label_encoder.fit_transform(Y)\n","\n","label_mappings = {encoded_label: original_label for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n","print(label_mappings)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y_encoded, test_size=0.2)\n","\n","waveforms_train = [spectrograms.load_from_path(f\"{FILEPATH}/{AUDIO_DIR}/{file}.mp3\") for file in X_train]\n","waveforms_test = [spectrograms.load_from_path(f\"{FILEPATH}/{AUDIO_DIR}/{file}.mp3\") for file in X_test]"]},{"cell_type":"markdown","metadata":{"id":"K_N2kwfnh25O"},"source":["### Standard Spectrogram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQQmRuouh25O"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","\n","\n","for n_fft in [512, 1024, 2048, 4096]:\n","    for win_length in [512, 1024, 2048, 4096]:\n","        if n_fft < win_length:\n","            continue\n","        else:\n","            os.makedirs(f\"{FILEPATH}/{DIR}/spectrograms/{n_fft}-{512}-{win_length}\", exist_ok=True)\n","            spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=512, fft=n_fft, win=win_length) for sample, sr in waveforms_train]]))\n","            spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=512, fft=n_fft, win=win_length) for sample, sr in waveforms_test]]))\n","            spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","            spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","            torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/spectrograms/{n_fft}-{512}-{win_length}/train.pt\")\n","            torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/spectrograms/{n_fft}-{512}-{win_length}/test.pt\")\n","\n","\n","for hop_length in [256, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/spectrograms/{2048}-{hop_length}-{2048}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/spectrograms/{2048}-{hop_length}-{2048}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/spectrograms/{2048}-{hop_length}-{2048}/test.pt\")\n"]},{"cell_type":"markdown","metadata":{"id":"bgYyIxHJh25O"},"source":["### Mel Spectrogram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejZ5v5tBh25O"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","\n","\n","for n_fft in [512, 1024, 2048, 4096]:\n","    for win_length in [512, 1024, 2048, 4096]:\n","        if n_fft < win_length:\n","            continue\n","        else:\n","            os.makedirs(f\"{FILEPATH}/{DIR}/mel_spectrograms/{n_fft}-{512}-{win_length}-{128}\", exist_ok=True)\n","            spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_train]]))\n","            spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_test]]))\n","            spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","            spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","            torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mel_spectrograms/{n_fft}-{512}-{win_length}-{128}/train.pt\")\n","            torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mel_spectrograms/{n_fft}-{512}-{win_length}-{128}/test.pt\")\n","\n","\n","for hop_length in [256, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{hop_length}-{2048}-{128}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{hop_length}-{2048}-{128}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{hop_length}-{2048}-{128}/test.pt\")\n","\n","\n","for mel_features in [64, 128, 192, 256]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{512}-{2048}-{mel_features}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{512}-{2048}-{mel_features}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{512}-{2048}-{mel_features}/test.pt\")"]},{"cell_type":"markdown","metadata":{"id":"_VOfBuf6h25P"},"source":["### MFCCs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1WeX2vLh25P"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","\n","for n_fft in [512, 1024, 2048, 4096]:\n","    for win_length in [512, 1024, 2048, 4096]:\n","        if n_fft < win_length:\n","            continue\n","        else:\n","            os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{n_fft}-{512}-{win_length}-{128}-{13}\", exist_ok=True)\n","            spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_train]]))\n","            spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_test]]))\n","            spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","            spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","            torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{n_fft}-{512}-{win_length}-{128}-{13}/train.pt\")\n","            torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{n_fft}-{512}-{win_length}-{128}-{13}/test.pt\")\n","\n","\n","for hop_length in [256, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{hop_length}-{2048}-{128}-{13}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{hop_length}-{2048}-{128}-{13}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{hop_length}-{2048}-{128}-{13}/test.pt\")\n","\n","\n","for mel_features in [64, 128, 192, 256]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{mel_features}-{13}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{mel_features}-{13}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{mel_features}-{13}/test.pt\")\n","\n","for mfcc_components in [5, 9, 13, 20]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{128}-{mfcc_components}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mfcc_bins=mfcc_components) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mfcc_bins=mfcc_components) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{128}-{mfcc_components}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{128}-{mfcc_components}/test.pt\")"]},{"cell_type":"markdown","metadata":{"id":"quC-qqvvh25P"},"source":["### CQT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYLOq4ruh25P"},"outputs":[],"source":["for hop_length in [256, 512, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/cqt_spectrograms/{hop_length}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.cqt_spectrogram(sample, sr, hop=hop_length) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.cqt_spectrogram(sample, sr, hop=hop_length) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/cqt_spectrograms/{hop_length}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/cqt_spectrograms/{hop_length}/test.pt\")"]},{"cell_type":"markdown","metadata":{"id":"v-ac1vMrh25P"},"source":["# ANN Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEiy6I9ph25P"},"outputs":[],"source":["def train(model, train_dataset, test_dataset, num_epochs, device):\n","    criterion = nn.CrossEntropyLoss()\n","    #optimizer = optim.SGD(model.parameters(), lr=0.01)\n","    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","    batch_size = 32\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    test_results = df()\n","    train_results = df()\n","\n","    epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        model.train()\n","        for inputs, targets in train_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            running_loss += loss.item()\n","\n","        epoch_progress_bar.update(1)\n","\n","        # Print average loss for the epoch\n","        test_results = test_network(model, test_dataset, criterion, test_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'test')\n","        train_results = test_network(model, train_dataset, criterion, test_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'train')\n","\n","    del model\n","    del inputs\n","    del targets\n","    del optimizer\n","    del criterion\n","    del loss\n","    gc.collect()\n","    if device == 'cuda': torch.cuda.empty_cache()\n","    elif device == 'mps': torch.mps.empty_cache()\n","\n","    print(\"Training finished!\")\n","    return test_results, train_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ha95p49VjS-I"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","FILEPATH = \"/content\"\n","\n","X = dataset[dataset['supercategory']=='music']['filename'].tolist()\n","Y = dataset[dataset['supercategory']=='music']['category'].map(lambda x: LABEL_MAPPINGS[x]).tolist()\n","\n","label_encoder = LabelEncoder()\n","Y_encoded = label_encoder.fit_transform(Y)\n","\n","label_mappings = {encoded_label: original_label for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n","print(label_mappings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JoO1V9rh25P"},"outputs":[],"source":["for spectrogram_type in ['mfcc_spectrograms', 'cqt_spectrograms', 'mel_spectrograms', 'spectrograms']:\n","  for spectrogram_files in os.listdir(f\"{FILEPATH}/{DIR}/{spectrogram_type}\"):\n","    if not os.path.isfile(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/train.pt\"):\n","      print(f\"{spectrogram_type}/{spectrogram_files}\")\n","      train_dataset = torch.load(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/train.pt\")\n","      test_dataset = torch.load(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/test.pt\")\n","\n","      x_shape = train_dataset[0][0].shape\n","      scale_factor = min(30000/(x_shape[0] * x_shape[1]), 1)\n","\n","      transform = transforms.Compose([\n","          transforms.ToTensor(),\n","          transforms.Resize(tuple(int(dim * scale_factor) for dim in x_shape), antialias=True)\n","      ])\n","\n","      train_dataset = [(transform(sample.numpy()), target) for sample, target in train_dataset]\n","      test_dataset = [(transform(sample.numpy()), target) for sample, target in test_dataset]\n","\n","      flattened_x_shape = int(x_shape[0]* scale_factor) * int(x_shape[1] * scale_factor)\n","\n","\n","      model = Triangle_Network(flattened_x_shape, len(label_encoder.classes_), beta=False).to(device)\n","      num_epochs = 120\n","\n","      criterion = nn.CrossEntropyLoss()\n","      #optimizer = optim.SGD(model.parameters(), lr=0.01)\n","      optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","      batch_size = 32\n","      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","      test_results = df()\n","      train_results = df()\n","\n","      epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","      for epoch in range(num_epochs):\n","          running_loss = 0.0\n","          model.train()\n","          for inputs, targets in train_loader:\n","              inputs, targets = inputs.to(device), targets.to(device)\n","\n","              outputs = model(inputs)\n","              loss = criterion(outputs, targets)\n","\n","              loss.backward()\n","              optimizer.step()\n","              optimizer.zero_grad()\n","\n","              running_loss += loss.item()\n","\n","          epoch_progress_bar.update(1)\n","\n","          # Print average loss for the epoch\n","          test_results = test_network(model, test_dataset, criterion, test_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'test')\n","          train_results = test_network(model, train_dataset, criterion, train_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'train')\n","\n","      del model\n","      del inputs\n","      del targets\n","      del optimizer\n","      del criterion\n","      del loss\n","      gc.collect()\n","      if device == 'cuda': torch.cuda.empty_cache()\n","      elif device == 'mps': torch.mps.empty_cache()\n","\n","      test_results.to_csv(f\"/content/drive/MyDrive/spectrogram_tensors/{spectrogram_type}/{spectrogram_files}/test.csv\")\n","      train_results.to_csv(f\"/content/drive/MyDrive/spectrogram_tensors/{spectrogram_type}/{spectrogram_files}/train.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLIP4saookhS"},"outputs":[],"source":["%reset"]},{"cell_type":"markdown","metadata":{},"source":["# Input Encodings"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from snntorch import functional as SF\n","\n","FILEPATH = \"../\"\n","TEST_TYPE = \"IST non-JNB results/input_encoding\"\n","SPECTROGRAMS = ['mel_spectrograms']"]},{"cell_type":"markdown","metadata":{},"source":["## Direct Encoding"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["ENCODING_TYPE = \"direct_encoding\""]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a9879551dc64002a5c4ad62f374c399","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.03656058166165761\n","Epoch 2 Running loss: 0.024120448368738234\n","Epoch 3 Running loss: 0.018885747877844812\n","Epoch 4 Running loss: 0.029438367510766533\n","Epoch 5 Running loss: 0.024561459337905503\n","Epoch 6 Running loss: 0.024297700364173613\n","Epoch 7 Running loss: 0.02726414395171189\n","Epoch 8 Running loss: 0.021354671330333086\n","Epoch 9 Running loss: 0.01863667377144346\n","Epoch 10 Running loss: 0.018206198129627513\n","Epoch 11 Running loss: 0.020360228097339745\n","Epoch 12 Running loss: 0.020451356829698727\n","Epoch 13 Running loss: 0.019991980365108585\n","Epoch 14 Running loss: 0.01848198766523451\n","Epoch 15 Running loss: 0.017847631116322864\n","Epoch 16 Running loss: 0.017939107900180974\n","Epoch 17 Running loss: 0.017240731669925256\n","Epoch 18 Running loss: 0.01675690376197202\n","Epoch 19 Running loss: 0.017660938141418628\n","Epoch 20 Running loss: 0.018239765946554676\n","Epoch 21 Running loss: 0.017519060264333794\n","Epoch 22 Running loss: 0.01785447194635703\n","Epoch 23 Running loss: 0.017842260424119943\n","Epoch 24 Running loss: 0.017136356506981677\n","Epoch 25 Running loss: 0.01710882609570786\n","Epoch 26 Running loss: 0.017013242039984285\n","Epoch 27 Running loss: 0.0168807777341383\n","Epoch 28 Running loss: 0.018335697723557743\n","Epoch 29 Running loss: 0.017597716931160797\n","Epoch 30 Running loss: 0.018666466847681272\n","Epoch 31 Running loss: 0.01698926314092409\n","Epoch 32 Running loss: 0.016041479612651625\n","Epoch 33 Running loss: 0.016629750708793998\n","Epoch 34 Running loss: 0.016925088916789132\n","Epoch 35 Running loss: 0.017774680314632002\n","Epoch 36 Running loss: 0.01801397694775272\n","Epoch 37 Running loss: 0.019295624418602095\n","Epoch 38 Running loss: 0.018314578196348574\n","Epoch 39 Running loss: 0.01939772965174963\n","Epoch 40 Running loss: 0.020523430568029345\n","Epoch 41 Running loss: 0.01824635878164022\n","Epoch 42 Running loss: 0.018270781826114392\n","Epoch 43 Running loss: 0.016667514626669423\n","Epoch 44 Running loss: 0.01652748333780389\n","Epoch 45 Running loss: 0.018788368748165564\n","Epoch 46 Running loss: 0.015955265539174594\n","Epoch 47 Running loss: 0.01605125444417515\n","Epoch 48 Running loss: 0.01611413850018192\n","Epoch 49 Running loss: 0.01667189069732074\n","Epoch 50 Running loss: 0.01615016150012241\n","Epoch 51 Running loss: 0.015524754563857313\n","Epoch 52 Running loss: 0.015857311497104464\n","Epoch 53 Running loss: 0.018443783894800413\n","Epoch 54 Running loss: 0.01645122373533381\n","Epoch 55 Running loss: 0.01822519566543875\n","Epoch 56 Running loss: 0.015243896156797119\n","Epoch 57 Running loss: 0.01684610004900565\n","Epoch 58 Running loss: 0.014751419466288137\n","Epoch 59 Running loss: 0.014958801692212387\n","Epoch 60 Running loss: 0.01445648676800926\n","Epoch 61 Running loss: 0.016002795702862938\n","Epoch 62 Running loss: 0.015835729332181556\n","Epoch 63 Running loss: 0.014894213702870208\n","Epoch 64 Running loss: 0.016709071777533956\n","Epoch 65 Running loss: 0.01480275309977439\n","Epoch 66 Running loss: 0.01579145018083567\n","Epoch 67 Running loss: 0.01445111838734381\n","Epoch 68 Running loss: 0.014777818214860319\n","Epoch 69 Running loss: 0.014765538997597312\n","Epoch 70 Running loss: 0.01666964718509579\n","Epoch 71 Running loss: 0.018359440845795945\n","Epoch 72 Running loss: 0.019069120824502114\n","Epoch 73 Running loss: 0.017646834130432467\n","Epoch 74 Running loss: 0.01887821226569094\n","Epoch 75 Running loss: 0.017859461234877316\n","Epoch 76 Running loss: 0.015611129454298362\n","Epoch 77 Running loss: 0.017377499067882423\n","Epoch 78 Running loss: 0.016733315182524706\n","Epoch 79 Running loss: 0.015293271587826208\n","Epoch 80 Running loss: 0.01571196994622989\n","Epoch 81 Running loss: 0.014790602337950815\n","Epoch 82 Running loss: 0.015115165314185653\n","Epoch 83 Running loss: 0.013912883161507815\n","Epoch 84 Running loss: 0.013393832045578891\n","Epoch 85 Running loss: 0.013788063770516097\n","Epoch 86 Running loss: 0.013172280755399666\n","Epoch 87 Running loss: 0.013356341219344628\n","Epoch 88 Running loss: 0.013139796719326536\n","Epoch 89 Running loss: 0.013464507964179126\n","Epoch 90 Running loss: 0.012582341719862496\n","Epoch 91 Running loss: 0.012762458040443484\n","Epoch 92 Running loss: 0.012756837041754471\n","Epoch 93 Running loss: 0.012661491074390359\n","Epoch 94 Running loss: 0.012400348100635814\n","Epoch 95 Running loss: 0.012572656047641406\n","Epoch 96 Running loss: 0.012726492168500483\n","Epoch 97 Running loss: 0.012526103003863812\n","Epoch 98 Running loss: 0.01341090407067719\n","Epoch 99 Running loss: 0.013225779639056514\n","Epoch 100 Running loss: 0.012055547613846628\n"]}],"source":["for spectrogram_type in SPECTROGRAMS:\n","    if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\"):\n","        print(f\"{spectrogram_type}\")\n","        train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")\n","        test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","        num_epochs = 100\n","\n","        criterion = criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        batch_size = 125\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","        train_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","    \n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            acc = 0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(1)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                to_print = (epoch_progress_bar if ((epoch+1) % 15 == 0) else None)\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.csv\")\n","        train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Time Contrast Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from delta import delta"]},{"cell_type":"markdown","metadata":{},"source":["## Direct Time Contrast"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"direct_TC_encoding\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in ['mfcc_spectrograms', 'mel_spectrograms']:\n","    original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","    original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","    direct_TC_test = delta(original_test_dataset, padding = True)\n","    direct_TC_train = delta(original_train_dataset, padding = True)\n","\n","    torch.save(direct_TC_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","    torch.save(direct_TC_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17024380b16a4a92837bc1b3b8b4be3a","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.00704795915605669\n","Epoch 2 Running loss: 0.008050204846805659\n","Epoch 3 Running loss: 0.007446746094920003\n","Epoch 4 Running loss: 0.007737645452903609\n","Epoch 5 Running loss: 0.007865204216954046\n","Epoch 6 Running loss: 0.010074967647385927\n","Epoch 7 Running loss: 0.010654191239573323\n","Epoch 8 Running loss: 0.01227419571881604\n","Epoch 9 Running loss: 0.011048067618990605\n","Epoch 10 Running loss: 0.009988871372291963\n","Epoch 11 Running loss: 0.00773311651552828\n","Epoch 12 Running loss: 0.007176489875720332\n","Epoch 13 Running loss: 0.0071892654553008155\n","Epoch 14 Running loss: 0.0070505622095955066\n","Epoch 15 Running loss: 0.007021990066138319\n","Epoch 16 Running loss: 0.006997302698251158\n","Epoch 17 Running loss: 0.006862228543867572\n","Epoch 18 Running loss: 0.0066147227637684\n","Epoch 19 Running loss: 0.006874226406613351\n","Epoch 20 Running loss: 0.006737658517937056\n","Epoch 21 Running loss: 0.006772403138133283\n","Epoch 22 Running loss: 0.00731804094121606\n","Epoch 23 Running loss: 0.00688625321474573\n","Epoch 24 Running loss: 0.00682373828908252\n","Epoch 25 Running loss: 0.007117686561121346\n","Epoch 26 Running loss: 0.006740412646081374\n","Epoch 27 Running loss: 0.006775361534378653\n","Epoch 28 Running loss: 0.006812587967568947\n","Epoch 29 Running loss: 0.0067419221226018835\n","Epoch 30 Running loss: 0.006645612696362253\n","Epoch 31 Running loss: 0.006595589228458425\n","Epoch 32 Running loss: 0.006500106903945549\n","Epoch 33 Running loss: 0.006475296645118787\n","Epoch 34 Running loss: 0.0059083318050169205\n","Epoch 35 Running loss: 0.00630644741403663\n","Epoch 36 Running loss: 0.006069354355906526\n","Epoch 37 Running loss: 0.005856626838667832\n","Epoch 38 Running loss: 0.006091658807537172\n","Epoch 39 Running loss: 0.006146121837587529\n","Epoch 40 Running loss: 0.00621493936727603\n","Epoch 41 Running loss: 0.005729071502360546\n","Epoch 42 Running loss: 0.0055306190119002955\n","Epoch 43 Running loss: 0.0056111732610879235\n","Epoch 44 Running loss: 0.005613168461508136\n","Epoch 45 Running loss: 0.006050571212118553\n","Epoch 46 Running loss: 0.005534083698504268\n","Epoch 47 Running loss: 0.005503231724991966\n","Epoch 48 Running loss: 0.005153835264130776\n","Epoch 49 Running loss: 0.005385144450032292\n","Epoch 50 Running loss: 0.005182764781549716\n","Epoch 51 Running loss: 0.0049758959121216605\n","Epoch 52 Running loss: 0.005057671072637946\n","Epoch 53 Running loss: 0.0049157166760315655\n","Epoch 54 Running loss: 0.004857233157173132\n","Epoch 55 Running loss: 0.00462185866789569\n","Epoch 56 Running loss: 0.004689491594942233\n","Epoch 57 Running loss: 0.004682643471942288\n","Epoch 58 Running loss: 0.0045388033087895956\n","Epoch 59 Running loss: 0.004785874273322515\n","Epoch 60 Running loss: 0.004853358157018757\n","Epoch 61 Running loss: 0.004397056354120517\n","Epoch 62 Running loss: 0.00458688540453601\n","Epoch 63 Running loss: 0.004624603909298508\n","Epoch 64 Running loss: 0.004360603956114878\n","Epoch 65 Running loss: 0.004576386481174392\n","Epoch 66 Running loss: 0.004395283322237805\n","Epoch 67 Running loss: 0.004071185764032431\n","Epoch 68 Running loss: 0.004377458042230088\n","Epoch 69 Running loss: 0.004179648570994092\n","Epoch 70 Running loss: 0.00392453015421907\n","Epoch 71 Running loss: 0.004212783040583959\n","Epoch 72 Running loss: 0.004290307545687317\n","Epoch 73 Running loss: 0.004415584067567088\n","Epoch 74 Running loss: 0.004086464992600381\n","Epoch 75 Running loss: 0.003946883609881416\n","Epoch 76 Running loss: 0.004336563098671845\n","Epoch 77 Running loss: 0.004061940252082671\n","Epoch 78 Running loss: 0.0039314101425349645\n","Epoch 79 Running loss: 0.003721484107077439\n","Epoch 80 Running loss: 0.0033695389922307577\n","Epoch 81 Running loss: 0.003696646426305476\n","Epoch 82 Running loss: 0.004073640916801997\n","Epoch 83 Running loss: 0.0038396565678020635\n","Epoch 84 Running loss: 0.003800825569957209\n","Epoch 85 Running loss: 0.0032184525800589172\n","Epoch 86 Running loss: 0.003592806502272146\n","Epoch 87 Running loss: 0.003175207347280809\n","Epoch 88 Running loss: 0.003442681064240087\n","Epoch 89 Running loss: 0.0032756783076114673\n","Epoch 90 Running loss: 0.0037158138581866017\n","Epoch 91 Running loss: 0.0031999719663422964\n","Epoch 92 Running loss: 0.0030128945050021203\n","Epoch 93 Running loss: 0.0029021411404188194\n","Epoch 94 Running loss: 0.0029760099574526787\n","Epoch 95 Running loss: 0.0030103000842979033\n","Epoch 96 Running loss: 0.00317688033984492\n","Epoch 97 Running loss: 0.0032523135407664145\n","Epoch 98 Running loss: 0.0036199515619978737\n","Epoch 99 Running loss: 0.0030772599676292672\n","Epoch 100 Running loss: 0.002693294907530276\n"]}],"source":["for spectrogram_type in SPECTROGRAMS:\n","    if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\"):\n","        print(f\"{spectrogram_type}\")\n","        train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")\n","        test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","        num_epochs = 100\n","\n","        criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        batch_size = 125\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","        train_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","    \n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            acc = 0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                # inputs in form of (time, batch, features)\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(0)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                to_print = (epoch_progress_bar if ((epoch+1) % 15 == 0) else None)\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.csv\")\n","        train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Time Contrast or Threshold Based"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["from delta import delta\n","\n","NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"time_contrast\"\n","\n","THRESHOLDS = [0.01, 0.025, 0.05, 0.10, 0.20, 0.50]\n","OFF_SPIKES = [True, False]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","            original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","            TC_test = delta(original_test_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True)\n","            TC_train = delta(original_train_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True)\n","\n","            os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}\", exist_ok=True)\n","\n","            torch.save(TC_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","            torch.save(TC_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.01_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19b66f17ca9e4a13bf61d52aab3ccc35","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.007166422594088716\n","Epoch 2 Running loss: 0.007144292132161296\n","Epoch 3 Running loss: 0.009545648059905909\n","Epoch 4 Running loss: 0.0072675961465500415\n","Epoch 5 Running loss: 0.007937676038224095\n","Epoch 6 Running loss: 0.006473075229519853\n","Epoch 7 Running loss: 0.006848833383843541\n","Epoch 8 Running loss: 0.008949163813179674\n","Epoch 9 Running loss: 0.00704632337672261\n","Epoch 10 Running loss: 0.00736007084861731\n","Epoch 11 Running loss: 0.01702452886599702\n","Epoch 12 Running loss: 0.006956521100319993\n","Epoch 13 Running loss: 0.007724476508058298\n","Epoch 14 Running loss: 0.006952628160056215\n","Epoch 15 Running loss: 0.008658721995429872\n","Epoch 16 Running loss: 0.00688813031671908\n","Epoch 17 Running loss: 0.00740758227273679\n","Epoch 18 Running loss: 0.010868007763506125\n","Epoch 19 Running loss: 0.006911721663734022\n","Epoch 20 Running loss: 0.00807946253889285\n","Epoch 21 Running loss: 0.005502318302853801\n","Epoch 22 Running loss: 0.006433545495755376\n","Epoch 23 Running loss: 0.006050253828493551\n","Epoch 24 Running loss: 0.010381314320305285\n","Epoch 25 Running loss: 0.007168715944686256\n","Epoch 26 Running loss: 0.010050417897038566\n","Epoch 27 Running loss: 0.006752075288242425\n","Epoch 28 Running loss: 0.005871522826508592\n","Epoch 29 Running loss: 0.0065526025363812435\n","Epoch 30 Running loss: 0.008888847911700654\n","Epoch 31 Running loss: 0.006894839933505073\n","Epoch 32 Running loss: 0.006512379684387304\n","Epoch 33 Running loss: 0.007145892697782181\n","Epoch 34 Running loss: 0.009453914797725007\n","Epoch 35 Running loss: 0.005333383624165203\n","Epoch 36 Running loss: 0.00789604009911656\n","Epoch 37 Running loss: 0.00863206919770652\n","Epoch 38 Running loss: 0.007298611413937407\n","Epoch 39 Running loss: 0.008838963584778026\n","Epoch 40 Running loss: 0.006680464592223731\n","Epoch 41 Running loss: 0.006366304600962435\n","Epoch 42 Running loss: 0.01002340575757499\n","Epoch 43 Running loss: 0.006411949285683921\n","Epoch 44 Running loss: 0.006093359031616308\n","Epoch 45 Running loss: 0.006936320481589808\n","Epoch 46 Running loss: 0.006746932340506166\n","Epoch 47 Running loss: 0.004904452955332427\n","Epoch 48 Running loss: 0.007254282411295004\n","Epoch 49 Running loss: 0.007300296720986168\n","Epoch 50 Running loss: 0.005269554952463022\n","Epoch 51 Running loss: 0.005915579228355481\n","Epoch 52 Running loss: 0.005166340464601121\n","Epoch 53 Running loss: 0.008434017435811198\n","Epoch 54 Running loss: 0.008477312688248606\n","Epoch 55 Running loss: 0.006214234013907825\n","Epoch 56 Running loss: 0.004994799189578992\n","Epoch 57 Running loss: 0.007508861085477348\n","Epoch 58 Running loss: 0.004562505517904751\n","Epoch 59 Running loss: 0.006338306699697964\n","Epoch 60 Running loss: 0.007165717335935599\n","Epoch 61 Running loss: 0.007107763815992556\n","Epoch 62 Running loss: 0.006100980808940558\n","Epoch 63 Running loss: 0.007759611827496903\n","Epoch 64 Running loss: 0.006352293796051805\n","Epoch 65 Running loss: 0.004338058384177022\n","Epoch 66 Running loss: 0.00470645035045763\n","Epoch 67 Running loss: 0.005154000160793146\n","Epoch 68 Running loss: 0.00895639339955851\n","Epoch 69 Running loss: 0.0050468429827842465\n","Epoch 70 Running loss: 0.00679589603274775\n","Epoch 71 Running loss: 0.004840899113053902\n","Epoch 72 Running loss: 0.006455133422114217\n","Epoch 73 Running loss: 0.00595807105588456\n","Epoch 74 Running loss: 0.006006482024543201\n","Epoch 75 Running loss: 0.004689523003647288\n","Epoch 76 Running loss: 0.005745320179211065\n","Epoch 77 Running loss: 0.005080604657959253\n","Epoch 78 Running loss: 0.0077190276342458996\n","Epoch 79 Running loss: 0.006478684207501884\n","Epoch 80 Running loss: 0.009797463401818808\n","Epoch 81 Running loss: 0.008539635818987227\n","Epoch 82 Running loss: 0.00997169787129655\n","Epoch 83 Running loss: 0.00774615127057694\n","Epoch 84 Running loss: 0.005025571447234756\n","Epoch 85 Running loss: 0.005648810344676788\n","Epoch 86 Running loss: 0.006183845737871651\n","Epoch 87 Running loss: 0.009080501981436635\n","Epoch 88 Running loss: 0.006319455159738803\n","Epoch 89 Running loss: 0.006544718346275841\n","Epoch 90 Running loss: 0.004605173841475869\n","Epoch 91 Running loss: 0.007707094232114359\n","Epoch 92 Running loss: 0.006427235972767059\n","Epoch 93 Running loss: 0.006483135703272713\n","Epoch 94 Running loss: 0.006294156987065324\n","Epoch 95 Running loss: 0.006959628944579786\n","Epoch 96 Running loss: 0.005678627390069322\n","Epoch 97 Running loss: 0.0062440951792195965\n","Epoch 98 Running loss: 0.007794051029430792\n","Epoch 99 Running loss: 0.005946051769744093\n","Epoch 100 Running loss: 0.005911910448211451\n","0.01_False_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97b6dfea7305405184be5cf3e2f9a48d","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.008424653079563055\n","Epoch 2 Running loss: 0.01465606917969335\n","Epoch 3 Running loss: 0.01029811385340584\n","Epoch 4 Running loss: 0.01074914981762822\n","Epoch 5 Running loss: 0.007254701072034744\n","Epoch 6 Running loss: 0.007754982660372798\n","Epoch 7 Running loss: 0.009934495432308307\n","Epoch 8 Running loss: 0.008964524291955624\n","Epoch 9 Running loss: 0.006860736840830063\n","Epoch 10 Running loss: 0.007216207956829772\n","Epoch 11 Running loss: 0.007871767012075113\n","Epoch 12 Running loss: 0.007097890011418742\n","Epoch 13 Running loss: 0.008403702665822575\n","Epoch 14 Running loss: 0.011796876074026188\n","Epoch 15 Running loss: 0.011365044040801808\n","Epoch 16 Running loss: 0.006399077181808484\n","Epoch 17 Running loss: 0.009209890715992108\n","Epoch 18 Running loss: 0.007937203200099567\n","Epoch 19 Running loss: 0.006854351907492446\n","Epoch 20 Running loss: 0.006464736530194267\n","Epoch 21 Running loss: 0.010295081252868946\n","Epoch 22 Running loss: 0.006929528741790845\n","Epoch 23 Running loss: 0.012594793932125592\n","Epoch 24 Running loss: 0.009108579958589694\n","Epoch 25 Running loss: 0.008616321383954618\n","Epoch 26 Running loss: 0.008387195035672416\n","Epoch 27 Running loss: 0.008440555284579342\n","Epoch 28 Running loss: 0.013424140576737376\n","Epoch 29 Running loss: 0.008052313956208885\n","Epoch 30 Running loss: 0.007596171511628757\n","Epoch 31 Running loss: 0.00805924371027718\n","Epoch 32 Running loss: 0.007966893739974537\n","Epoch 33 Running loss: 0.00670924697059412\n","Epoch 34 Running loss: 0.013307971314500315\n","Epoch 35 Running loss: 0.007034257387581725\n","Epoch 36 Running loss: 0.0071061036457268955\n","Epoch 37 Running loss: 0.008212875634336625\n","Epoch 38 Running loss: 0.006405534073948479\n","Epoch 39 Running loss: 0.007342142514146555\n","Epoch 40 Running loss: 0.010005658998276098\n","Epoch 41 Running loss: 0.010011958999755665\n","Epoch 42 Running loss: 0.009754833893273205\n","Epoch 43 Running loss: 0.010144073932696455\n","Epoch 44 Running loss: 0.009550908693490318\n","Epoch 45 Running loss: 0.009281300127315826\n","Epoch 46 Running loss: 0.009073457397972814\n","Epoch 47 Running loss: 0.006439750472577616\n","Epoch 48 Running loss: 0.007153165987886179\n","Epoch 49 Running loss: 0.007278826099615127\n","Epoch 50 Running loss: 0.007708006773512965\n","Epoch 51 Running loss: 0.008508304247079185\n","Epoch 52 Running loss: 0.008505623751935868\n","Epoch 53 Running loss: 0.01791213076716414\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.007019886955285605\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.007019886955285605\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.007019886955285605\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.007019886955285605\n","Epoch 67 Running loss: 0.011115110720308444\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.005306848896125826\n","Epoch 84 Running loss: 0.013263310868138322\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.01256405469327689\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.025_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bafedd87d6a4631ba84db3b05ccdf67","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.0060268576248004415\n","Epoch 2 Running loss: 0.009104535983393367\n","Epoch 3 Running loss: 0.011397028121704492\n","Epoch 4 Running loss: 0.009872320551460924\n","Epoch 5 Running loss: 0.006753364143470606\n","Epoch 6 Running loss: 0.010088732257818643\n","Epoch 7 Running loss: 0.006875221102763288\n","Epoch 8 Running loss: 0.006143645737498713\n","Epoch 9 Running loss: 0.006756224714148159\n","Epoch 10 Running loss: 0.009183625634105061\n","Epoch 11 Running loss: 0.008481685917217511\n","Epoch 12 Running loss: 0.00856708928038137\n","Epoch 13 Running loss: 0.007008080046397809\n","Epoch 14 Running loss: 0.009820476888467709\n","Epoch 15 Running loss: 0.00755622935371277\n","Epoch 16 Running loss: 0.006523139536761628\n","Epoch 17 Running loss: 0.007426456807139582\n","Epoch 18 Running loss: 0.01106194928050422\n","Epoch 19 Running loss: 0.010685291534033827\n","Epoch 20 Running loss: 0.005793796465419733\n","Epoch 21 Running loss: 0.006564866001613605\n","Epoch 22 Running loss: 0.008985942354598365\n","Epoch 23 Running loss: 0.00881728710838781\n","Epoch 24 Running loss: 0.0065688854589249\n","Epoch 25 Running loss: 0.005953955478942432\n","Epoch 26 Running loss: 0.009262539898625579\n","Epoch 27 Running loss: 0.005329506013530512\n","Epoch 28 Running loss: 0.007307579913459266\n","Epoch 29 Running loss: 0.006808357402539482\n","Epoch 30 Running loss: 0.0055357398102268245\n","Epoch 31 Running loss: 0.007086041636360339\n","Epoch 32 Running loss: 0.006970685415755446\n","Epoch 33 Running loss: 0.006418463140250014\n","Epoch 34 Running loss: 0.005481710925269812\n","Epoch 35 Running loss: 0.006893202995720763\n","Epoch 36 Running loss: 0.00749740223534191\n","Epoch 37 Running loss: 0.007251398822370048\n","Epoch 38 Running loss: 0.01026700537044781\n","Epoch 39 Running loss: 0.005871779669207125\n","Epoch 40 Running loss: 0.007456989048388057\n","Epoch 41 Running loss: 0.006023643877559577\n","Epoch 42 Running loss: 0.005353347561991633\n","Epoch 43 Running loss: 0.004864103544634371\n","Epoch 44 Running loss: 0.007272869919816526\n","Epoch 45 Running loss: 0.009310622756092693\n","Epoch 46 Running loss: 0.005025336655755394\n","Epoch 47 Running loss: 0.005802632949222772\n","Epoch 48 Running loss: 0.006953639249070384\n","Epoch 49 Running loss: 0.007496408951549104\n","Epoch 50 Running loss: 0.010317730351377981\n","Epoch 51 Running loss: 0.0052530650918285685\n","Epoch 52 Running loss: 0.005282920913193554\n","Epoch 53 Running loss: 0.008204837291004559\n","Epoch 54 Running loss: 0.007537330491855122\n","Epoch 55 Running loss: 0.008243694092138126\n","Epoch 56 Running loss: 0.008230324846487076\n","Epoch 57 Running loss: 0.006050003508028512\n","Epoch 58 Running loss: 0.006122772305156476\n","Epoch 59 Running loss: 0.010952437647615379\n","Epoch 60 Running loss: 0.007243633270263672\n","Epoch 61 Running loss: 0.009043280404215804\n","Epoch 62 Running loss: 0.00483289791848332\n","Epoch 63 Running loss: 0.006214552127515165\n","Epoch 64 Running loss: 0.005786950976703876\n","Epoch 65 Running loss: 0.007717906952666017\n","Epoch 66 Running loss: 0.0067901267601659125\n","Epoch 67 Running loss: 0.005771340987534997\n","Epoch 68 Running loss: 0.005598556690703566\n","Epoch 69 Running loss: 0.005835129144473579\n","Epoch 70 Running loss: 0.007423123707786535\n","Epoch 71 Running loss: 0.006553105748118684\n","Epoch 72 Running loss: 0.0059486256239894095\n","Epoch 73 Running loss: 0.007037019577270117\n","Epoch 74 Running loss: 0.006852390661169164\n","Epoch 75 Running loss: 0.006113461793040315\n","Epoch 76 Running loss: 0.01701210824826274\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.008569017385903257\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.005306848896125826\n","Epoch 100 Running loss: 0.013862726025688001\n","0.025_False_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c3989ea607245bfa481efcef792b1f7","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.029947224135596914\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.00819628059673614\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.009300459307222701\n","Epoch 17 Running loss: 0.009784627265442675\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.011207293397702349\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.007019886955285605\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.011931359196623293\n","Epoch 30 Running loss: 0.007019886955285605\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.0075494637504553265\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.007019886955285605\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.01074137226842082\n","Epoch 44 Running loss: 0.007019886955285605\n","Epoch 45 Running loss: 0.007019886955285605\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.01101527046471739\n","Epoch 48 Running loss: 0.008634857857189239\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.007019886955285605\n","Epoch 51 Running loss: 0.007019886955285605\n","Epoch 52 Running loss: 0.007019886955285605\n","Epoch 53 Running loss: 0.007019886955285605\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.008918881797181151\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.006735708290776505\n","Epoch 60 Running loss: 0.007589399814605713\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.008569017385903257\n","Epoch 64 Running loss: 0.00798328806417057\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.007019886955285605\n","Epoch 67 Running loss: 0.007019886955285605\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.011931359196623293\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.05_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4cbab4964b44e649bedcbfc034276c6","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.02355249792623063\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.007019886955285605\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.007019886955285605\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.007019886955285605\n","Epoch 30 Running loss: 0.007019886955285605\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.007019886955285605\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.007019886955285605\n","Epoch 44 Running loss: 0.007019886955285605\n","Epoch 45 Running loss: 0.007609367751465819\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007019886955285605\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.007019886955285605\n","Epoch 51 Running loss: 0.007019886955285605\n","Epoch 52 Running loss: 0.010072931314047915\n","Epoch 53 Running loss: 0.010317482506505217\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.007019886955285605\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.007019886955285605\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.007019886955285605\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.007019886955285605\n","Epoch 67 Running loss: 0.007019886955285605\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.009212929600724777\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.009300459307222701\n","Epoch 87 Running loss: 0.008601577518085322\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.05_False_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7cf6760f3cc4f35b639a1bd06cda573","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.030011086227794807\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.007019886955285605\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.015966238686071037\n","Epoch 27 Running loss: 0.01223750274402265\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.007019886955285605\n","Epoch 30 Running loss: 0.007019886955285605\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.007019886955285605\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.007019886955285605\n","Epoch 44 Running loss: 0.008116408278005192\n","Epoch 45 Running loss: 0.007019886955285605\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007019886955285605\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.007019886955285605\n","Epoch 51 Running loss: 0.007019886955285605\n","Epoch 52 Running loss: 0.007019886955285605\n","Epoch 53 Running loss: 0.007019886955285605\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.01108082909934437\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.007019886955285605\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.008569017385903257\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.008626017136314806\n","Epoch 67 Running loss: 0.007019886955285605\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.00975800322267575\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.007928856645529262\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.012623959075147732\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.011372973743719034\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007469591431724378\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.010948710167369903\n","Epoch 96 Running loss: 0.008143032320772117\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.1_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a26ae84c3ab425fb0bec45d19391fa9","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.03836603134204024\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.010216548419988956\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.007019886955285605\n","Epoch 18 Running loss: 0.008418497186118422\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.010249828759092873\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.007019886955285605\n","Epoch 30 Running loss: 0.007019886955285605\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.007019886955285605\n","Epoch 41 Running loss: 0.011832238767093745\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.007019886955285605\n","Epoch 44 Running loss: 0.007019886955285605\n","Epoch 45 Running loss: 0.007019886955285605\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007019886955285605\n","Epoch 49 Running loss: 0.008089784235238267\n","Epoch 50 Running loss: 0.007019886955285605\n","Epoch 51 Running loss: 0.007019886955285605\n","Epoch 52 Running loss: 0.007019886955285605\n","Epoch 53 Running loss: 0.007019886955285605\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.011081830381204525\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.009598258966074203\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.007019886955285605\n","Epoch 65 Running loss: 0.008222904639503065\n","Epoch 66 Running loss: 0.007019886955285605\n","Epoch 67 Running loss: 0.007019886955285605\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.01117401343945878\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.0075494637504553265\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.011931359577483643\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007569431687315432\n","Epoch 97 Running loss: 0.010249828378232522\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.1_False_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f0dbcb91cf941788cfff2675de132d1","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.014143049716949463\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.007019886955285605\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.007019886955285605\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.007019886955285605\n","Epoch 30 Running loss: 0.018862682790421068\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.007019886955285605\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.007019886955285605\n","Epoch 44 Running loss: 0.007928856645529262\n","Epoch 45 Running loss: 0.007019886955285605\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007019886955285605\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.012510806607743041\n","Epoch 51 Running loss: 0.007019886955285605\n","Epoch 52 Running loss: 0.007019886955285605\n","Epoch 53 Running loss: 0.007019886955285605\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.007019886955285605\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.007019886955285605\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.007019886955285605\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.007019886955285605\n","Epoch 67 Running loss: 0.008566112754443965\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.008569017385903257\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.013210062401744124\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.008569017385903257\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.011932079403545149\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.012623959075147732\n","Epoch 99 Running loss: 0.008569017385903257\n","Epoch 100 Running loss: 0.007019886955285605\n","0.2_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12afbbfa609c4d9ab599d100d550d931","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.03727307563391737\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.010915430209126335\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.008009912106937495\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.01084886991177885\n","Epoch 21 Running loss: 0.00798328806417057\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.007019886955285605\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.011521501663013007\n","Epoch 30 Running loss: 0.008668137815432807\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.008089784235238267\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.007019886955285605\n","Epoch 44 Running loss: 0.007019886955285605\n","Epoch 45 Running loss: 0.007019886955285605\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007589399814605713\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.007222060447398085\n","Epoch 51 Running loss: 0.008116408278005192\n","Epoch 52 Running loss: 0.007019886955285605\n","Epoch 53 Running loss: 0.007019886955285605\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.007019886955285605\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.007019886955285605\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.007019886955285605\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.007208730656498918\n","Epoch 67 Running loss: 0.007019886955285605\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.008569017385903257\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.008999937258589382\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.015497291050018213\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.012597334651520457\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.2_False_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfe43db6c18e4efeb2a8cd10d7c65d29","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.008890442573986114\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.007019886955285605\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.007019886955285605\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.007019886955285605\n","Epoch 30 Running loss: 0.007019886955285605\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.007019886955285605\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.007019886955285605\n","Epoch 44 Running loss: 0.007019886955285605\n","Epoch 45 Running loss: 0.007019886955285605\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007019886955285605\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.007019886955285605\n","Epoch 51 Running loss: 0.007019886955285605\n","Epoch 52 Running loss: 0.009300459307222701\n","Epoch 53 Running loss: 0.007019886955285605\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.009300459307222701\n","Epoch 57 Running loss: 0.008634857857189239\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.01131306974270854\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.008116408278005192\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.007019886955285605\n","Epoch 67 Running loss: 0.007019886955285605\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.008569017385903257\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.01058262929367943\n","Epoch 84 Running loss: 0.008116408278005192\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.010721404331560713\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.01240712575638256\n","Epoch 94 Running loss: 0.008934378623962402\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007469591431724378\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.5_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1854fe4b26724ab5b9534bfaf44151cc","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.0348658957801307\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.007019886955285605\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.007019886955285605\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.007019886955285605\n","Epoch 30 Running loss: 0.007019886955285605\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.007019886955285605\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.007019886955285605\n","Epoch 43 Running loss: 0.007019886955285605\n","Epoch 44 Running loss: 0.007019886955285605\n","Epoch 45 Running loss: 0.007019886955285605\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007019886955285605\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.007019886955285605\n","Epoch 51 Running loss: 0.007019886955285605\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# inputs in form of (time, batch, features)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 42\u001b[0m     spikes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(spikes, targets)\n\u001b[1;32m     46\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[22], line 37\u001b[0m, in \u001b[0;36mTriangle_Network.<locals>.Net.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m cur2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk1)\n\u001b[1;32m     36\u001b[0m spk2, mem2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif2(cur2, mem2)\n\u001b[0;32m---> 37\u001b[0m cur3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspk2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m spk3, mem3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif3(cur3, mem3)\n\u001b[1;32m     39\u001b[0m cur4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(spk3)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\"):\n","                print(f\"{threshold}_{off_spike}_{spectrogram_type}\")\n","                train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")\n","                test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","\n","                # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","                x_shape = train_dataset[0][0].shape\n","\n","                # Assuming the shape is t x f\n","                features_shape = x_shape[1]\n","                POP_ENCODING = 10\n","                classes = len(label_encoder.classes_)\n","                output_shape = classes * POP_ENCODING\n","\n","\n","                model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","                num_epochs = 100\n","\n","                criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","                optimizer = optim.Adam(model.parameters(), lr=0.005)\n","\n","                batch_size = 120\n","                train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","                test_results = df()\n","                train_results = df()\n","\n","                epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","                for epoch in range(num_epochs):\n","                    running_loss = 0.0\n","                    total = 0\n","                    model.train()\n","                    for inputs, targets in train_loader:\n","                        # inputs in form of (time, batch, features)\n","                        inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                        spikes, _ = model(inputs)\n","\n","                        loss = criterion(spikes, targets)\n","\n","                        loss.backward()\n","                        optimizer.step()\n","                        optimizer.zero_grad()\n","\n","                        running_loss += loss.item()\n","                        total += spikes.size(0)\n","\n","                    epoch_progress_bar.update(1)\n","\n","                    print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                    \n","                    # Print average loss for the epoch\n","                    if ((epoch+1) % 5 == 0):\n","                        test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                        train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","                del model\n","                del inputs\n","                del targets\n","                del optimizer\n","                del criterion\n","                del loss\n","                gc.collect()\n","                if device == 'cuda': torch.cuda.empty_cache()\n","                elif device == 'mps': torch.mps.empty_cache()\n","\n","                test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.csv\")\n","                train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Cumulative Time Contrast (SF)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["from delta import delta\n","\n","NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"step_forward\"\n","\n","THRESHOLDS = [0.01, 0.025, 0.05, 0.10, 0.20, 0.50]\n","OFF_SPIKES = [True, False]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","            original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","            TC_test = delta(original_test_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True, cumulative=True)\n","            TC_train = delta(original_train_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True, cumulative=True)\n","\n","            os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}\", exist_ok=True)\n","\n","            torch.save(TC_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","            torch.save(TC_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.01_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd869563b6f54d61aefbc5947066e703","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.007901065837080105\n","Epoch 2 Running loss: 0.007652933986042255\n","Epoch 3 Running loss: 0.01084291668364796\n","Epoch 4 Running loss: 0.0076840411359890585\n","Epoch 5 Running loss: 0.011170355085366832\n","Epoch 6 Running loss: 0.00695115365921118\n","Epoch 7 Running loss: 0.007276734985863439\n","Epoch 8 Running loss: 0.010692866084674678\n","Epoch 9 Running loss: 0.009105912031837927\n","Epoch 10 Running loss: 0.0075143098640746584\n","Epoch 11 Running loss: 0.010604738046566899\n","Epoch 12 Running loss: 0.007785448536705286\n","Epoch 13 Running loss: 0.00900059786086646\n","Epoch 14 Running loss: 0.011345365176947352\n","Epoch 15 Running loss: 0.0070808757417879925\n","Epoch 16 Running loss: 0.007597232588563864\n","Epoch 17 Running loss: 0.005805750148364911\n","Epoch 18 Running loss: 0.008159414647867123\n","Epoch 19 Running loss: 0.005886874926356843\n","Epoch 20 Running loss: 0.007873300355844224\n","Epoch 21 Running loss: 0.006321666625361092\n","Epoch 22 Running loss: 0.008794323324014584\n","Epoch 23 Running loss: 0.00657647372053835\n","Epoch 24 Running loss: 0.007158128026956186\n","Epoch 25 Running loss: 0.00772707188091339\n","Epoch 26 Running loss: 0.007310128916566745\n","Epoch 27 Running loss: 0.008206274467535293\n","Epoch 28 Running loss: 0.006506073970002488\n","Epoch 29 Running loss: 0.007703056541113808\n","Epoch 30 Running loss: 0.00663151832434316\n","Epoch 31 Running loss: 0.005038403736326261\n","Epoch 32 Running loss: 0.006870910287284242\n","Epoch 33 Running loss: 0.008170983090568275\n","Epoch 34 Running loss: 0.008323713803824526\n","Epoch 35 Running loss: 0.007539515582898173\n","Epoch 36 Running loss: 0.007566497253533751\n","Epoch 37 Running loss: 0.006697390216608017\n","Epoch 38 Running loss: 0.00663996675905709\n","Epoch 39 Running loss: 0.005242975160908014\n","Epoch 40 Running loss: 0.008278081973139851\n","Epoch 41 Running loss: 0.009122040134649307\n","Epoch 42 Running loss: 0.005356881374749132\n","Epoch 43 Running loss: 0.005920206038906171\n","Epoch 44 Running loss: 0.006814142004750407\n","Epoch 45 Running loss: 0.008543050327240087\n","Epoch 46 Running loss: 0.006897808739933343\n","Epoch 47 Running loss: 0.005822450969927608\n","Epoch 48 Running loss: 0.005460110240089246\n","Epoch 49 Running loss: 0.005006057863010647\n","Epoch 50 Running loss: 0.0068626305737053625\n","Epoch 51 Running loss: 0.006816605219064048\n","Epoch 52 Running loss: 0.005966962288362911\n","Epoch 53 Running loss: 0.007913496738043837\n","Epoch 54 Running loss: 0.006670960222189419\n","Epoch 55 Running loss: 0.006840629699512031\n","Epoch 56 Running loss: 0.007536387100768166\n","Epoch 57 Running loss: 0.009090685330259914\n","Epoch 58 Running loss: 0.005506205577819873\n","Epoch 59 Running loss: 0.005357447761697129\n","Epoch 60 Running loss: 0.008851442283715684\n","Epoch 61 Running loss: 0.005003121714241589\n","Epoch 62 Running loss: 0.006215662716295772\n","Epoch 63 Running loss: 0.0062507910850329905\n","Epoch 64 Running loss: 0.005111823352381063\n","Epoch 65 Running loss: 0.005472389558633676\n","Epoch 66 Running loss: 0.0042790842596620035\n","Epoch 67 Running loss: 0.0051000070648071485\n","Epoch 68 Running loss: 0.005875319242477417\n","Epoch 69 Running loss: 0.005502234442165485\n","Epoch 70 Running loss: 0.0045504448132012215\n","Epoch 71 Running loss: 0.005195315843953873\n","Epoch 72 Running loss: 0.0053892745948828065\n","Epoch 73 Running loss: 0.005494850226484549\n","Epoch 74 Running loss: 0.008574359142742217\n","Epoch 75 Running loss: 0.004285951320546123\n","Epoch 76 Running loss: 0.004392409738831627\n","Epoch 77 Running loss: 0.005427243800970693\n","Epoch 78 Running loss: 0.004601314092596499\n","Epoch 79 Running loss: 0.005690771169936695\n","Epoch 80 Running loss: 0.0072022213722570255\n","Epoch 81 Running loss: 0.004706175039751461\n","Epoch 82 Running loss: 0.008360095869618864\n","Epoch 83 Running loss: 0.006589250633130058\n","Epoch 84 Running loss: 0.005648122427943415\n","Epoch 85 Running loss: 0.008119844019222564\n","Epoch 86 Running loss: 0.005880252907451349\n","Epoch 87 Running loss: 0.0050539660948914844\n","Epoch 88 Running loss: 0.007527739666521358\n","Epoch 89 Running loss: 0.005770034397752902\n","Epoch 90 Running loss: 0.00790703753693797\n","Epoch 91 Running loss: 0.0057289315679202825\n","Epoch 92 Running loss: 0.006109019009449992\n","Epoch 93 Running loss: 0.0068721122825488494\n","Epoch 94 Running loss: 0.005719259781495165\n","Epoch 95 Running loss: 0.005030557965508665\n","Epoch 96 Running loss: 0.0070325508475684514\n","Epoch 97 Running loss: 0.004212602877769226\n","Epoch 98 Running loss: 0.006764573791918282\n","Epoch 99 Running loss: 0.00502546279194256\n","Epoch 100 Running loss: 0.005409653860920915\n","0.01_False_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69a344751e6d4f098e2fe8f0e32c7ffd","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.007170156739390315\n","Epoch 2 Running loss: 0.008055452340707992\n","Epoch 3 Running loss: 0.011519777127348196\n","Epoch 4 Running loss: 0.012959768216069133\n","Epoch 5 Running loss: 0.008156594376975355\n","Epoch 6 Running loss: 0.009099299153580834\n","Epoch 7 Running loss: 0.010270625828934935\n","Epoch 8 Running loss: 0.0077131270600583985\n","Epoch 9 Running loss: 0.014270476830272247\n","Epoch 10 Running loss: 0.008639533956425069\n","Epoch 11 Running loss: 0.010708788427681968\n","Epoch 12 Running loss: 0.008444471671558417\n","Epoch 13 Running loss: 0.00784523970783709\n","Epoch 14 Running loss: 0.006259219465069116\n","Epoch 15 Running loss: 0.021333553730108486\n","Epoch 16 Running loss: 0.007125161992856108\n","Epoch 17 Running loss: 0.005365267407875091\n","Epoch 18 Running loss: 0.005890868020990786\n","Epoch 19 Running loss: 0.008405490995596012\n","Epoch 20 Running loss: 0.007429361152953614\n","Epoch 21 Running loss: 0.00820689403210966\n","Epoch 22 Running loss: 0.01045600960429865\n","Epoch 23 Running loss: 0.006752448531385428\n","Epoch 24 Running loss: 0.011438018026443335\n","Epoch 25 Running loss: 0.006658252530966323\n","Epoch 26 Running loss: 0.005662126115526254\n","Epoch 27 Running loss: 0.006645470667190064\n","Epoch 28 Running loss: 0.008478368052278465\n","Epoch 29 Running loss: 0.008782982254942385\n","Epoch 30 Running loss: 0.009007595979367582\n","Epoch 31 Running loss: 0.007936402631643863\n","Epoch 32 Running loss: 0.007952841516500844\n","Epoch 33 Running loss: 0.007218251974818806\n","Epoch 34 Running loss: 0.008892892839047855\n","Epoch 35 Running loss: 0.007344729127213597\n","Epoch 36 Running loss: 0.007425249575045162\n","Epoch 37 Running loss: 0.008867243608346762\n","Epoch 38 Running loss: 0.008377563648711377\n","Epoch 39 Running loss: 0.012378666252373887\n","Epoch 40 Running loss: 0.006592922412549345\n","Epoch 41 Running loss: 0.009438825110657908\n","Epoch 42 Running loss: 0.006493261637398229\n","Epoch 43 Running loss: 0.005937470414768012\n","Epoch 44 Running loss: 0.007078175346691387\n","Epoch 45 Running loss: 0.007247072534439282\n","Epoch 46 Running loss: 0.0067967231186053245\n","Epoch 47 Running loss: 0.007063591323150232\n","Epoch 48 Running loss: 0.012425663562628407\n","Epoch 49 Running loss: 0.005999785785477002\n","Epoch 50 Running loss: 0.007636077297381319\n","Epoch 51 Running loss: 0.013159770744677168\n","Epoch 52 Running loss: 0.00572096282681718\n","Epoch 53 Running loss: 0.005770288839245947\n","Epoch 54 Running loss: 0.008016144505704934\n","Epoch 55 Running loss: 0.00706433326291581\n","Epoch 56 Running loss: 0.0063164425543702826\n","Epoch 57 Running loss: 0.009003953240549983\n","Epoch 58 Running loss: 0.008382104265804107\n","Epoch 59 Running loss: 0.00601380166059104\n","Epoch 60 Running loss: 0.009874962960569241\n","Epoch 61 Running loss: 0.00970158066612463\n","Epoch 62 Running loss: 0.005839331652790594\n","Epoch 63 Running loss: 0.00882269132632417\n","Epoch 64 Running loss: 0.005721825285079761\n","Epoch 65 Running loss: 0.005854278945694335\n","Epoch 66 Running loss: 0.007979300075445693\n","Epoch 67 Running loss: 0.0068647492046173385\n","Epoch 68 Running loss: 0.006427741564881687\n","Epoch 69 Running loss: 0.005321106588551268\n","Epoch 70 Running loss: 0.008336749320593886\n","Epoch 71 Running loss: 0.007027334488999729\n","Epoch 72 Running loss: 0.01472538747726538\n","Epoch 73 Running loss: 0.005006242174500474\n","Epoch 74 Running loss: 0.010632720808632458\n","Epoch 75 Running loss: 0.006813152148700751\n","Epoch 76 Running loss: 0.006293325378491094\n","Epoch 77 Running loss: 0.007500925193579433\n","Epoch 78 Running loss: 0.0068780916948288015\n","Epoch 79 Running loss: 0.011713648185181542\n","Epoch 80 Running loss: 0.007617695358233711\n","Epoch 81 Running loss: 0.009103243724225808\n","Epoch 82 Running loss: 0.00692598735943389\n","Epoch 83 Running loss: 0.00673408487353462\n","Epoch 84 Running loss: 0.006523617445089566\n","Epoch 85 Running loss: 0.00662947719851241\n","Epoch 86 Running loss: 0.007971866347919257\n","Epoch 87 Running loss: 0.005557643148464898\n","Epoch 88 Running loss: 0.00834728106142233\n","Epoch 89 Running loss: 0.006987636176922832\n","Epoch 90 Running loss: 0.006726470427772108\n","Epoch 91 Running loss: 0.007251886418833138\n","Epoch 92 Running loss: 0.004981578086702207\n","Epoch 93 Running loss: 0.007835670591543277\n","Epoch 94 Running loss: 0.00821935349759964\n","Epoch 95 Running loss: 0.008915614586668655\n","Epoch 96 Running loss: 0.006144099645673657\n","Epoch 97 Running loss: 0.009379107350358566\n","Epoch 98 Running loss: 0.006595326974369086\n","Epoch 99 Running loss: 0.005648560274523287\n","Epoch 100 Running loss: 0.008509673440037445\n","0.025_True_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dc878a32813406296b595fbf139d361","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.007165805981182062\n","Epoch 2 Running loss: 0.009618258133483009\n","Epoch 3 Running loss: 0.006041306710709779\n","Epoch 4 Running loss: 0.010902753272376503\n","Epoch 5 Running loss: 0.008401230882151058\n","Epoch 6 Running loss: 0.007008250052936542\n","Epoch 7 Running loss: 0.008174532518600123\n","Epoch 8 Running loss: 0.006497843435016303\n","Epoch 9 Running loss: 0.010740051444727011\n","Epoch 10 Running loss: 0.00863593359724782\n","Epoch 11 Running loss: 0.008614198277933529\n","Epoch 12 Running loss: 0.007419147716162685\n","Epoch 13 Running loss: 0.00683482434041203\n","Epoch 14 Running loss: 0.008279725575980288\n","Epoch 15 Running loss: 0.007063486991218104\n","Epoch 16 Running loss: 0.0070740083774057825\n","Epoch 17 Running loss: 0.006818920849992064\n","Epoch 18 Running loss: 0.007929234078136114\n","Epoch 19 Running loss: 0.006953525847901171\n","Epoch 20 Running loss: 0.00647545993899385\n","Epoch 21 Running loss: 0.00787645711685522\n","Epoch 22 Running loss: 0.012799561404572508\n","Epoch 23 Running loss: 0.008667248696945727\n","Epoch 24 Running loss: 0.007020574217787185\n","Epoch 25 Running loss: 0.009987370845989678\n","Epoch 26 Running loss: 0.00872638812080359\n","Epoch 27 Running loss: 0.006999088171571969\n","Epoch 28 Running loss: 0.007412538741724179\n","Epoch 29 Running loss: 0.006844219924233402\n","Epoch 30 Running loss: 0.009751097081949154\n","Epoch 31 Running loss: 0.009389659657645912\n","Epoch 32 Running loss: 0.008484181885521252\n","Epoch 33 Running loss: 0.008573205326311886\n","Epoch 34 Running loss: 0.006881346527379923\n","Epoch 35 Running loss: 0.007396331610390172\n","Epoch 36 Running loss: 0.006100659315197612\n","Epoch 37 Running loss: 0.009782796088879863\n","Epoch 38 Running loss: 0.008483303621554146\n","Epoch 39 Running loss: 0.006360416690381571\n","Epoch 40 Running loss: 0.007866408878241103\n","Epoch 41 Running loss: 0.009064176592963952\n","Epoch 42 Running loss: 0.007570646155756503\n","Epoch 43 Running loss: 0.00804814039327847\n","Epoch 44 Running loss: 0.016875479929744246\n","Epoch 45 Running loss: 0.009078584540004547\n","Epoch 46 Running loss: 0.007019886955285605\n","Epoch 47 Running loss: 0.007019886955285605\n","Epoch 48 Running loss: 0.007019886955285605\n","Epoch 49 Running loss: 0.007019886955285605\n","Epoch 50 Running loss: 0.007019886955285605\n","Epoch 51 Running loss: 0.007019886955285605\n","Epoch 52 Running loss: 0.007019886955285605\n","Epoch 53 Running loss: 0.007019886955285605\n","Epoch 54 Running loss: 0.007019886955285605\n","Epoch 55 Running loss: 0.007019886955285605\n","Epoch 56 Running loss: 0.007019886955285605\n","Epoch 57 Running loss: 0.008601577518085322\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.007019886955285605\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.007019886955285605\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.013849039809010661\n","Epoch 67 Running loss: 0.007019886955285605\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.00795666383097347\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.009300459307222701\n","Epoch 78 Running loss: 0.008701417773676376\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.007019886955285605\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.01057486496709537\n","Epoch 88 Running loss: 0.013879097307832858\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.007019886955285605\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.007019886955285605\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n","0.025_False_mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b31d36db194d4a1eb837972faae3c15b","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.027861988963410498\n","Epoch 2 Running loss: 0.007019886955285605\n","Epoch 3 Running loss: 0.007019886955285605\n","Epoch 4 Running loss: 0.007019886955285605\n","Epoch 5 Running loss: 0.007019886955285605\n","Epoch 6 Running loss: 0.007019886955285605\n","Epoch 7 Running loss: 0.007019886955285605\n","Epoch 8 Running loss: 0.007019886955285605\n","Epoch 9 Running loss: 0.007019886955285605\n","Epoch 10 Running loss: 0.007019886955285605\n","Epoch 11 Running loss: 0.007019886955285605\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.007019886955285605\n","Epoch 14 Running loss: 0.007019886955285605\n","Epoch 15 Running loss: 0.007019886955285605\n","Epoch 16 Running loss: 0.007019886955285605\n","Epoch 17 Running loss: 0.007019886955285605\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007019886955285605\n","Epoch 20 Running loss: 0.007019886955285605\n","Epoch 21 Running loss: 0.007019886955285605\n","Epoch 22 Running loss: 0.007019886955285605\n","Epoch 23 Running loss: 0.01101527046471739\n","Epoch 24 Running loss: 0.007019886955285605\n","Epoch 25 Running loss: 0.007019886955285605\n","Epoch 26 Running loss: 0.008116408278005192\n","Epoch 27 Running loss: 0.007019886955285605\n","Epoch 28 Running loss: 0.007019886955285605\n","Epoch 29 Running loss: 0.007019886955285605\n","Epoch 30 Running loss: 0.011125518110232612\n","Epoch 31 Running loss: 0.007019886955285605\n","Epoch 32 Running loss: 0.007019886955285605\n","Epoch 33 Running loss: 0.007019886955285605\n","Epoch 34 Running loss: 0.007019886955285605\n","Epoch 35 Running loss: 0.007019886955285605\n","Epoch 36 Running loss: 0.007019886955285605\n","Epoch 37 Running loss: 0.007019886955285605\n","Epoch 38 Running loss: 0.007019886955285605\n","Epoch 39 Running loss: 0.007019886955285605\n","Epoch 40 Running loss: 0.008009912106937495\n","Epoch 41 Running loss: 0.007019886955285605\n","Epoch 42 Running loss: 0.009292801919455727\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m spikes, _ \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(spikes, targets)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\"):\n","                print(f\"{threshold}_{off_spike}_{spectrogram_type}\")\n","                train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")\n","                test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","\n","                # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","                x_shape = train_dataset[0][0].shape\n","\n","                # Assuming the shape is t x f\n","                features_shape = x_shape[1]\n","                POP_ENCODING = 10\n","                classes = len(label_encoder.classes_)\n","                output_shape = classes * POP_ENCODING\n","\n","\n","                model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","                num_epochs = 100\n","\n","                criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","                optimizer = optim.Adam(model.parameters(), lr=0.005)\n","\n","                batch_size = 120\n","                train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","                test_results = df()\n","                train_results = df()\n","\n","                epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","                for epoch in range(num_epochs):\n","                    running_loss = 0.0\n","                    total = 0\n","                    model.train()\n","                    for inputs, targets in train_loader:\n","                        # inputs in form of (time, batch, features)\n","                        inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                        spikes, _ = model(inputs)\n","\n","                        loss = criterion(spikes, targets)\n","\n","                        loss.backward()\n","                        optimizer.step()\n","                        optimizer.zero_grad()\n","\n","                        running_loss += loss.item()\n","                        total += spikes.size(0)\n","\n","                    epoch_progress_bar.update(1)\n","\n","                    print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                    \n","                    # Print average loss for the epoch\n","                    if ((epoch+1) % 5 == 0):\n","                        test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                        train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","                del model\n","                del inputs\n","                del targets\n","                del optimizer\n","                del criterion\n","                del loss\n","                gc.collect()\n","                if device == 'cuda': torch.cuda.empty_cache()\n","                elif device == 'mps': torch.mps.empty_cache()\n","\n","                test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.csv\")\n","                train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Rate Encoding Imports"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["from rate import rate, count"]},{"cell_type":"markdown","metadata":{},"source":["## Count Encoding (Whole Spectrogram)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"count_encoding\"\n","SUB = \"whole\"\n","\n","for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","        original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","        poisson_test = count(original_test_dataset, max_spikes=n_count)\n","        poisson_train = count(original_train_dataset, max_spikes=n_count)\n","\n","        os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}\", exist_ok=True)\n","\n","        torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","        torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\"):\n","            print(f\"{spectrogram_type}\")\n","            train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")\n","            test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","\n","            x_shape = train_dataset[0][0].shape\n","            scale_factor = min(30000/(x_shape[0] * x_shape[1]), 1)\n","\n","            transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(tuple(int(dim * scale_factor) for dim in x_shape), antialias=True)\n","            ])\n","\n","            train_dataset = [(transform(sample.numpy()), target) for sample, target in train_dataset]\n","            test_dataset = [(transform(sample.numpy()), target) for sample, target in test_dataset]\n","\n","            flattened_x_shape = int(x_shape[0]* scale_factor) * int(x_shape[1] * scale_factor)\n","\n","            features_shape = flattened_x_shape\n","            POP_ENCODING = 10\n","            classes = len(label_encoder.classes_)\n","            output_shape = classes * POP_ENCODING\n","\n","\n","            model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","            num_epochs = 100\n","\n","            criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","            optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","            batch_size = 120\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","            test_results = df()\n","\n","            epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","            for epoch in range(num_epochs):\n","                running_loss = 0.0\n","                total = 0\n","                model.train()\n","                for inputs, targets in train_loader:\n","                    # inputs in form of (time, batch, features)\n","                    inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                    spikes, _ = model(inputs)\n","\n","                    loss = criterion(spikes, targets)\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    running_loss += loss.item()\n","                    total += spikes.size(0)\n","\n","                epoch_progress_bar.update(1)\n","\n","                print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                \n","                # Print average loss for the epoch\n","                if ((epoch+1) % 5 == 0):\n","                    test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                    train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","            del model\n","            del inputs\n","            del targets\n","            del optimizer\n","            del criterion\n","            del loss\n","            gc.collect()\n","            if device == 'cuda': torch.cuda.empty_cache()\n","            elif device == 'mps': torch.mps.empty_cache()\n","\n","            test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.csv\")\n","            train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Count Encoding (Extending Dims)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"count_encoding\"\n","SUB = \"extend\"\n","\n","for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","        original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","        poisson_test = count(original_test_dataset, max_spikes=n_count, time_varying=True)\n","        poisson_train = count(original_train_dataset, max_spikes=n_count, time_varying=True)\n","\n","        os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}\", exist_ok=True)\n","\n","        torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","        torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\"):\n","            print(f\"{spectrogram_type}\")\n","            train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")\n","            test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","\n","            # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","            x_shape = train_dataset[0][0].shape\n","\n","            # Assuming the shape is t x f\n","            features_shape = x_shape[1]\n","            POP_ENCODING = 10\n","            classes = len(label_encoder.classes_)\n","            output_shape = classes * POP_ENCODING\n","\n","\n","            model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","            num_epochs = 120\n","\n","            criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","            optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","            batch_size = 120\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","            test_results = df()\n","\n","            epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","            for epoch in range(num_epochs):\n","                running_loss = 0.0\n","                total = 0\n","                model.train()\n","                for inputs, targets in train_loader:\n","                    # inputs in form of (time, batch, features)\n","                    inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                    spikes, _ = model(inputs)\n","\n","                    loss = criterion(spikes, targets)\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    running_loss += loss.item()\n","                    total += spikes.size(0)\n","\n","                epoch_progress_bar.update(1)\n","\n","                print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                \n","                # Print average loss for the epoch\n","                if ((epoch+1) % 5 == 0):\n","                    test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                    train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","            del model\n","            del inputs\n","            del targets\n","            del optimizer\n","            del criterion\n","            del loss\n","            gc.collect()\n","            if device == 'cuda': torch.cuda.empty_cache()\n","            elif device == 'mps': torch.mps.empty_cache()\n","\n","            test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.csv\")\n","            train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Poisson Encoding (Whole Spectrogram)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"poisson_count\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","        original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","        poisson_test = rate(original_test_dataset, extend=False, num_steps=n_count)\n","        poisson_train = rate(original_train_dataset, extend=False, num_steps=n_count)\n","\n","        os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}\", exist_ok=True)\n","\n","        torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","        torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\"):\n","            print(f\"{spectrogram_type}\")\n","            train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")\n","            test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","\n","            x_shape = train_dataset[0][0].shape\n","            scale_factor = min(30000/(x_shape[0] * x_shape[1]), 1)\n","\n","            transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(tuple(int(dim * scale_factor) for dim in x_shape), antialias=True)\n","            ])\n","\n","            train_dataset = [(transform(sample.numpy()), target) for sample, target in train_dataset]\n","            test_dataset = [(transform(sample.numpy()), target) for sample, target in test_dataset]\n","\n","            flattened_x_shape = int(x_shape[0]* scale_factor) * int(x_shape[1] * scale_factor)\n","\n","            features_shape = flattened_x_shape\n","            POP_ENCODING = 10\n","            classes = len(label_encoder.classes_)\n","            output_shape = classes * POP_ENCODING\n","\n","\n","            model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","            num_epochs = 100\n","\n","            criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","            optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","            batch_size = 120\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","            test_results = df()\n","\n","            epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","            for epoch in range(num_epochs):\n","                running_loss = 0.0\n","                total = 0\n","                model.train()\n","                for inputs, targets in train_loader:\n","                    # inputs in form of (time, batch, features)\n","                    inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                    spikes, _ = model(inputs)\n","\n","                    loss = criterion(spikes, targets)\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    running_loss += loss.item()\n","                    total += spikes.size(0)\n","\n","                epoch_progress_bar.update(1)\n","\n","                print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                \n","                # Print average loss for the epoch\n","                if ((epoch+1) % 5 == 0):\n","                    test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                    train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","            del model\n","            del inputs\n","            del targets\n","            del optimizer\n","            del criterion\n","            del loss\n","            gc.collect()\n","            if device == 'cuda': torch.cuda.empty_cache()\n","            elif device == 'mps': torch.mps.empty_cache()\n","\n","            test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.csv\")\n","            train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Poisson Encoding (Keeping Dimensions)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"poisson_dims\""]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","    original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","    poisson_test = rate(original_test_dataset)\n","    poisson_train = rate(original_train_dataset)\n","\n","    os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}\", exist_ok=True)\n","\n","    torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","    torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mel_spectrograms\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68961d551e954f63be7bd3c208c88998","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.021674252165773046\n","Epoch 2 Running loss: 0.007019886955285605\n"]}],"source":["for spectrogram_type in SPECTROGRAMS:\n","    if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\"):\n","        print(f\"{spectrogram_type}\")\n","        train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")\n","        test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","        num_epochs = 100\n","\n","        criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        batch_size = 120\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                # inputs in form of (time, batch, features)\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(0)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.csv\")\n","        train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Poisson Encoding (Extending Dims)"]},{"cell_type":"markdown","metadata":{},"source":["### Note that this took too long for Count Encoding (Extending Dims) --- i.e. it takes upwards of 50hrs to train Count_n=15"]},{"cell_type":"markdown","metadata":{},"source":["## Autoencoder"]},{"cell_type":"markdown","metadata":{},"source":["# Architectures & Action Potentials"]},{"cell_type":"markdown","metadata":{},"source":["## MLP (Triangle)"]},{"cell_type":"markdown","metadata":{},"source":["## Convolutional Neural Network"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["QrnOeGaHh25K","43m8jiuhh25L","st4_dGj4h25N"],"gpuType":"A100","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
