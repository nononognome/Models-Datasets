{"cells":[{"cell_type":"markdown","metadata":{"id":"cHy2H4YVh25D"},"source":["# Basic Setup and Functions"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rXuB5FHah25G"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Using mps device\n"]}],"source":["%pip install snntorch --quiet\n","\n","import librosa, random\n","import numpy as np\n","import pandas as pd\n","import os\n","import soundfile as sf\n","\n","from pandas import DataFrame as df\n","import torch\n","\n","from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","import snntorch as snn\n","from snntorch.functional.acc import _population_code, _prediction_check\n","\n","import torch.nn as nn\n","from torch import Tensor\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch import optim\n","from torchvision import transforms\n","\n","from tqdm.notebook import tqdm\n","\n","import gc\n","\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3ae_oxqGh25H"},"outputs":[],"source":["def Triangle_Network(num_inputs, num_outputs, beta=0.90, time_dependent = False):\n","    dy_dx = int(4/(num_outputs - num_inputs))\n","    hidden1 = num_inputs + (dy_dx * 1)\n","    hidden2 = num_inputs + (dy_dx * 2)\n","    hidden3 = num_inputs + (dy_dx * 3)\n","\n","    if beta and time_dependent:\n","        class Net(nn.Module):\n","        # Initialise network with 2 forward connections (linear connections) and 2 leaky integrated fire layers (hidden and output)\n","            def __init__(self, *args, **kwargs) -> None:\n","                super().__init__(*args, **kwargs)\n","                self.fc1 = nn.Linear(num_inputs, hidden1)\n","                self.lif1 = snn.Leaky(beta=beta)\n","                self.fc2 = nn.Linear(hidden1, hidden2)\n","                self.lif2 = snn.Leaky(beta=beta)\n","                self.fc3 = nn.Linear(hidden3, hidden3)\n","                self.lif3 = snn.Leaky(beta=beta)\n","                self.fc4 = nn.Linear(hidden3, num_outputs)\n","                self.lif4 = snn.Leaky(beta=beta)\n","\n","            # Define a forward pass assuming x is normalised data (i.e. all values in [0,1])\n","            def forward(self, x):\n","                mem1 = self.lif1.init_leaky()\n","                mem2 = self.lif2.init_leaky()\n","                mem3 = self.lif3.init_leaky()\n","                mem4 = self.lif4.init_leaky()\n","\n","                spk_rec = []\n","                mem_rec = []\n","\n","                # Insert data in shape (time x batch x features)\n","                for step in range(x.size(0)):\n","                    cur1 = self.fc1(x[step])\n","                    spk1, mem1 = self.lif1(cur1, mem1)\n","                    cur2 = self.fc2(spk1)\n","                    spk2, mem2 = self.lif2(cur2, mem2)\n","                    cur3 = self.fc3(spk2)\n","                    spk3, mem3 = self.lif3(cur3, mem3)\n","                    cur4 = self.fc4(spk3)\n","                    spk4, mem4 = self.lif4(cur4, mem4)\n","\n","                    spk_rec.append(spk4)\n","                    mem_rec.append(mem4)\n","\n","                return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)\n","            \n","        return Net()\n","\n","\n","    elif beta and not time_dependent: return nn.Sequential(nn.Flatten(),\n","                    nn.Linear(num_inputs, hidden1),\n","                    snn.Leaky(beta=beta, init_hidden=True),\n","                    nn.Linear(hidden1, hidden2),\n","                    snn.Leaky(beta=beta, init_hidden=True),\n","                    nn.Linear(hidden2, hidden3),\n","                    snn.Leaky(beta=beta, init_hidden=True),\n","                    nn.Linear(hidden3, num_outputs),\n","                    snn.Leaky(beta=beta, init_hidden=True, output=True))\n","\n","    else: return nn.Sequential(nn.Flatten(),\n","                    nn.Linear(num_inputs, hidden1),\n","                    nn.ReLU(),\n","                    nn.Linear(hidden1, hidden2),\n","                    nn.ReLU(),\n","                    nn.Linear(hidden2, hidden3),\n","                    nn.ReLU(),\n","                    nn.Linear(hidden3, num_outputs))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_6kx2Evvh25I"},"outputs":[],"source":["def test_spiking_network(model, dataset, loss_fn, results: df, epoch, device, num_classes=False, printable=None, train_test = 'test'):\n","    dataloader = DataLoader(dataset, batch_size=100, num_workers=3, shuffle=False)\n","    model.eval()\n","    with torch.no_grad():\n","        test_loss = 0.0\n","        correct = 0\n","        total = 0\n","        total_spikes = 0\n","        all_labels = []\n","        all_predicted = []\n","        all_probs = []\n","\n","        for data, labels in dataloader:\n","            x, labels = data.transpose(0, 1).to(device), labels.to(device)\n","            spikes, _ = model(x)\n","            test_loss += loss_fn(spikes, labels).item()\n","            \n","            if num_classes: _, predicted = _population_code(spikes, num_classes=num_classes, num_outputs=spikes.size(-1)).max(1)\n","            else: _, predicted = spikes.sum(dim=1).max(1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predicted.extend(predicted.cpu().numpy())\n","\n","            if num_classes: num_spikes = _population_code(spikes, num_classes=num_classes, num_outputs=spikes.size(-1))\n","            else: num_spikes = spikes.sum(dim=1)\n","            \n","            softmax = torch.nn.Softmax(dim=1)\n","            probabilities = softmax(num_spikes)\n","            all_probs.extend(probabilities.cpu().numpy())\n","        \n","            total_spikes += spikes.size(1)\n","\n","        test_loss /= total_spikes\n","\n","    # Accuracy\n","    accuracy = 100 * correct / total\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(all_labels, all_predicted)\n","\n","    # Recall/Sensitivity -- avoiding div by 0\n","    recall = recall_score(all_labels, all_predicted, average='weighted', zero_division=0) * 100\n","\n","    # Precision\n","    precision = precision_score(all_labels, all_predicted, average='weighted', zero_division=0) * 100\n","\n","    # F1 Score\n","    f1_score = (2 * precision * recall) / (precision + recall)\n","\n","    # AUC-ROC\n","    auc_roc = 100 * roc_auc_score(all_labels, all_probs, multi_class='ovr')\n","    \n","    if printable: printable.set_description(\n","        f'Epoch [{epoch + 1}] {train_test} Loss: {test_loss / len(dataloader):.2f} '\n","        f'{train_test} Accuracy: {accuracy:.2f}% F1: {f1_score}% Recall: {recall:.2f}% Precision: {precision:.2f}% '\n","        f'AUC-ROC: {auc_roc:.4f}%'\n","    )\n","\n","    results = results._append({\n","            'Epoch': epoch + 1,\n","            'Accuracy': accuracy,\n","            'F1': f1_score,\n","            'Recall': recall,\n","            'Precision': precision,\n","            'Test Loss': test_loss / len(dataloader),\n","            'AUC-ROC': auc_roc,\n","            'Confusion Matrix': cm\n","        }, ignore_index=True)\n","\n","    del data\n","    del labels\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    return results"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class PopulationCrossEntropyLoss():\n","    def __init__(self, num_classes=2):\n","        self.num_classes = num_classes\n","        self.__name__ = \"PopulationCrossEntropyLoss\"\n","\n","    def __call__(self, spk_out, targets):\n","        loss_fn = nn.CrossEntropyLoss()\n","        \n","        _, _, num_outputs = _prediction_check(spk_out)\n","\n","        spike_count = _population_code(\n","                spk_out, self.num_classes, num_outputs\n","            )\n","\n","        loss = loss_fn(spike_count, targets)\n","\n","        return loss\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"q93J30hEh25J"},"outputs":[],"source":["LABEL_MAPPINGS = {\n","    'westernart/classical': 'Classical',\n","    'indierock/pop': 'Rock',\n","    'pop/soul/electronica': 'Electronic',\n","    'electronica': 'Electronic',\n","    'jazz': 'Jazz',\n","    'pop/hiphop/rock': 'Pop',\n","    'rap/hiphop': 'Hiphop',\n","    'rock': 'Rock',\n","    'rock/folk': 'Rock',\n","    'westernart/baroque': 'Classical',\n","    'electronica/dance': 'Electronic',\n","    'westernart/romantic': 'Classical',\n","    'blues': 'Jazz',\n","    'pop/folk': 'Pop',\n","    'westernart/romantic/classical': 'Classical',\n","    'pop/electronica': 'Electronic',\n","    'latin': 'Jazz',\n","    'country/folk': 'Country',\n","    'indierock/folk/pop': 'Rock',\n","    'jazz/blues': 'Jazz',\n","    'pop/rap/rock/hiphop': 'Pop',\n","    'pop/experimental': 'Pop',\n","    'blues/rock/jazz': 'Jazz',\n","    'jazz/adventure': 'Jazz',\n","    'blues/electronica': 'Jazz',\n","    'jazz/pop/soul': 'Jazz',\n","    'funk/electronica': 'Electronic',\n","    'folk/pop': 'Folk',\n","    'indierock/rock': 'Rock',\n","    'jazz/electronica': 'Electronic',\n","    'hiphop': 'Hiphop',\n","    'funk/rnb/adventure': 'Soul',\n","    'pop': 'Pop',\n","    'hiphop/rap': 'Hiphop',\n","    'pop/gospel': 'Soul',\n","    'rap/metal/electronica': 'Electronic',\n","    'pop/rock/folk': 'Rock',\n","    'pop/electronica/hiphop': 'Pop',\n","    'metal/rap': 'Hiphop',\n","    'country': 'Country',\n","    'rap/metal': 'Hiphop',\n","    'country/pop': 'Country',\n","    'folk': 'Folk',\n","    'pop/rock/dance': 'Pop',\n","    'dance': 'Electronic',\n","    'pop/jazz/latin': 'Jazz',\n","    'pop/jazz': 'Jazz',\n","    'funk/rnb/electronica': 'Electronic',\n","    'funk/blues/jazz': 'Jazz',\n","    'pop/rock/soul': 'Pop',\n","    'pop/hiphop': 'Pop',\n","    'blues/funk': 'Jazz',\n","    'rap/metal/hiphop': 'Hiphop',\n","    'blues/jazz/adventure': 'Jazz',\n","    'folk/indierock': 'Folk',\n","    'adventure': 'Classical',\n","    'metal/rock': 'Rock',\n","    'blues/rock/country': 'Jazz',\n","    'pop/soul/rnb': 'Soul',\n","    'blues/rock': 'Jazz',\n","    'blues/rock/indierock': 'Jazz',\n","    'country/pop/folk': 'Country',\n","    'country/blues/rock': 'Country',\n","    'rock/funk/country': 'Rock',\n","    'pop/rock': 'Rock',\n","    'pop/blues': 'Rock',\n","    'blues/indierock': 'Rock',\n","    'blues/rock/rnb': 'Rock',\n","    'blues/pop/folk': 'Jazz',\n","    'pop/funk/adventure': 'Pop',\n","    'blues/rock/pop': 'Rock',\n","    'folk/pop/funk': 'Folk'\n","}\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"y5-_se-YiO7P"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 'Classical', 1: 'Country', 2: 'Electronic', 3: 'Folk', 4: 'Hiphop', 5: 'Jazz', 6: 'Pop', 7: 'Rock', 8: 'Soul'}\n"]}],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    !rsync -av --exclude='mel_spectrograms' --exclude='cqt_spectrograms' /content/drive/MyDrive/spectrogram_tensors/ /content/spectrogram_tensors/ --quiet\n","    FILEPATH = '/content'\n","    CSV = 'sample_ISD.csv'\n","\n","    dataset = pd.read_csv(f'/content/drive/MyDrive/{CSV}', index_col=0)\n","\n","    \n","except:\n","    FILEPATH = \"../../Datasets/SmallDataset\"\n","    ORIGINAL_DIR = \"audio\"\n","    SAMPLE_DIR = \"audio uncompressed samples\"\n","    COMPRESSED_DIR = \"audio compressed\"\n","    CSV = 'sample_ISD.csv'\n","\n","    dataset = pd.read_csv(f'{FILEPATH}/{CSV}', index_col=0)\n","\n","X = dataset['filename'].tolist()\n","Y = dataset[dataset['supercategory']=='music']['category'].map(lambda x: LABEL_MAPPINGS[x]).tolist()\n","label_encoder = LabelEncoder()\n","Y_encoded = label_encoder.fit_transform(Y)\n","\n","label_mappings = {encoded_label: original_label for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n","print(label_mappings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Garbage collection special commands\n","\n","gc.collect()\n","if device == \"cuda\":\n","    torch.cuda.empty_cache()\n","    torch.cuda.memory_summary(device=None, abbreviated=False)\n","elif device == \"mps\":\n","    torch.mps.empty_cache()\n","    print(f\"MPS occupied memory: {torch.mps.driver_allocated_memory()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Very special command -- remove all variables\n","%reset"]},{"cell_type":"markdown","metadata":{"id":"JjzopoGZh25J"},"source":["# Audio Representation"]},{"cell_type":"markdown","metadata":{"id":"QrnOeGaHh25K"},"source":["## Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qroeHpJh25L"},"outputs":[],"source":["def sample(audio_path, duration=5.0, sr=44100):\n","    original_path = audio_path\n","    for ext in [\".m4a\", \".wav\", \".ogg\", \".flac\", \".mp3\"]:\n","        if os.path.exists(audio_path + ext):\n","            audio_path += ext\n","\n","            total_duration = librosa.get_duration(path=audio_path)\n","            y, _ = librosa.load(audio_path, sr=sr, duration=total_duration)\n","\n","            if total_duration < duration:\n","                pad_length = int((duration - total_duration) * sr)\n","                y = np.pad(y, (0, pad_length), mode='constant')\n","\n","            start = random.uniform(0, max(0, total_duration - duration))\n","            y = y[int(start * sr):int((start + duration) * sr)]\n","\n","            sf.write(f\"{FILEPATH}/{SAMPLE_DIR}/{original_path.split('/')[-1]}.wav\", y, sr)\n","\n","            return y"]},{"cell_type":"markdown","metadata":{"id":"43m8jiuhh25L"},"source":["## Bitrate and Compression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74ui8zpqh25M"},"outputs":[],"source":["import os\n","from tqdm.notebook import tqdm\n","from concurrent.futures import ThreadPoolExecutor\n","\n","def convert_to_mp3(input_file, output_file, sample_rate=16000, bit_rate=\"8k\", channels=1):\n","    !ffmpeg -i \"$input_file\" -ar \"$sample_rate\" -b:a \"$bitrate\" -ac \"$channels\" \"$output_file\" -hide_banner -loglevel error\n","\n","\n","def convert_directory_to_mp3(input_dir, output_dir, sample_rate=16000, bit_rate=\"8k\"):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    audio_files = [file for file in os.listdir(input_dir) if file.endswith((\".m4a\", \".wav\", \".ogg\", \".flac\", \".mp3\"))]\n","\n","    for file in tqdm(audio_files, desc=\"Converting\"):\n","            input_file_path = os.path.join(input_dir, file)\n","            output_file_path = os.path.join(output_dir, os.path.splitext(file)[0] + \".mp3\")\n","            convert_to_mp3(input_file_path, output_file_path, sample_rate, bit_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAUf5u8Yh25M"},"outputs":[],"source":["bitdepths = np.array([2,4,8,16,24])\n","samplerates = np.int32(np.array([8,16,22.05,32,44.1])*1000)\n","\n","with ThreadPoolExecutor() as executor:\n","    for bitdepth in bitdepths:\n","        for samplerate in samplerates:\n","            bitrate = (bitdepth * samplerate) / 1000\n","            print(f\"bitdepth: {bitdepth}, samplerate: {samplerate}\")\n","            print(f\"effective bitrate: {bitrate} kbps\")\n","\n","            executor.submit(convert_directory_to_mp3(f\"{FILEPATH}/{SAMPLE_DIR}\", f\"{FILEPATH}/{COMPRESSED_DIR}/{bitdepth}-{samplerate}\", sample_rate=samplerate, bit_rate=f\"{bitrate}k\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEmydtY2h25M"},"outputs":[],"source":["# Assuming the directory contains all compressed files\n","!find . -mindepth 1 -maxdepth 1 -type d -exec sh -c 'find \"$1\" -type f -exec ls -l {} \\; | awk \"{sum += \\$5} END {print \\\"$1\\\", sum}\"' _ {} \\"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0K_lIJiLh25N"},"outputs":[],"source":["from mutagen.mp3 import MP3\n","\n","def get_unpacked_size(mp3_file):\n","    audio = MP3(mp3_file)\n","    duration = audio.info.length  # Duration of the audio in seconds\n","    bitrate = audio.info.bitrate  # Bitrate of the audio in bits per second\n","    # Calculate the unpacked size based on bitrate and duration\n","    unpacked_size = (duration * bitrate) / 8\n","    return unpacked_size\n","\n","\n","file_dirs = [d for d in os.listdir(f\"{FILEPATH}/{COMPRESSED_DIR}\") if os.path.isdir(f\"{FILEPATH}/{COMPRESSED_DIR}/{d}\")]\n","for bitrate in file_dirs:\n","    audio_files = [file for file in os.listdir(f\"{FILEPATH}/{COMPRESSED_DIR}/{bitrate}\") if file.endswith((\".mp3\"))]\n","    size = 0\n","    for file in audio_files:\n","        if os.path.exists(f\"{FILEPATH}/{COMPRESSED_DIR}/{bitrate}/{file}\"):\n","            size += get_unpacked_size(f\"{FILEPATH}/{COMPRESSED_DIR}/{bitrate}/{file}\")\n","    print(f\"{bitrate.split('/')[-1]}: {size} bytes\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BW6hyuwDh25N"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import tikzplotlib\n","\n","bit_two = np.array([[16,32,44.1,64,88.2],[1,3,4,5,5]]).T\n","bit_four = np.array([[32,64,88.2,128,176.4],[2,4,5,5,5]]).T\n","bit_eight = np.array([[64,128,176.4,256,352.8],[2,4,5,5,5]]).T\n","bit_sixteen = np.array([[128,256,352.8,512,705.6],[2,3,4,4,5]]).T\n","bit_twentyfour = np.array([[192,384,529.2,768,1058.4],[2,4,5,5,5]]).T\n","\n","plt.plot(bit_two[:,0], bit_two[:,1], label=\"2-bit\")\n","plt.plot(bit_four[:,0], bit_four[:,1], label=\"4-bit\")\n","plt.plot(bit_eight[:,0], bit_eight[:,1], label=\"8-bit\")\n","plt.plot(bit_sixteen[:,0], bit_sixteen[:,1], label=\"16-bit\")\n","plt.plot(bit_twentyfour[:,0], bit_twentyfour[:,1], label=\"24-bit\")\n","\n","plt.ylabel(\"Perceived Quality\")\n","plt.xlabel(\"Bitrate (kbps)\")\n","plt.xscale(\"log\")\n","plt.xlim(10,1100)\n","plt.ylim(0, 6)\n","plt.grid(True, which='both', axis='y')\n","#plt.legend()\n","\n","tikzplotlib.save(\"AudioRep/CompressionReception.tex\")"]},{"cell_type":"markdown","metadata":{"id":"st4_dGj4h25N"},"source":["## Spectrograms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yE7h8re8h25O"},"outputs":[],"source":["import spectrograms\n","\n","AUDIO_DIR = \"compressed_audio\"\n","\n","X = dataset[dataset['supercategory']=='music']['filename'].tolist()\n","Y = dataset[dataset['supercategory']=='music']['category'].map(lambda x: LABEL_MAPPINGS[x]).tolist()\n","\n","label_encoder = LabelEncoder()\n","Y_encoded = label_encoder.fit_transform(Y)\n","\n","label_mappings = {encoded_label: original_label for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n","print(label_mappings)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y_encoded, test_size=0.2)\n","\n","waveforms_train = [spectrograms.load_from_path(f\"{FILEPATH}/{AUDIO_DIR}/{file}.mp3\") for file in X_train]\n","waveforms_test = [spectrograms.load_from_path(f\"{FILEPATH}/{AUDIO_DIR}/{file}.mp3\") for file in X_test]"]},{"cell_type":"markdown","metadata":{"id":"K_N2kwfnh25O"},"source":["### Standard Spectrogram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQQmRuouh25O"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","\n","\n","for n_fft in [512, 1024, 2048, 4096]:\n","    for win_length in [512, 1024, 2048, 4096]:\n","        if n_fft < win_length:\n","            continue\n","        else:\n","            os.makedirs(f\"{FILEPATH}/{DIR}/spectrograms/{n_fft}-{512}-{win_length}\", exist_ok=True)\n","            spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=512, fft=n_fft, win=win_length) for sample, sr in waveforms_train]]))\n","            spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=512, fft=n_fft, win=win_length) for sample, sr in waveforms_test]]))\n","            spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","            spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","            torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/spectrograms/{n_fft}-{512}-{win_length}/train.pt\")\n","            torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/spectrograms/{n_fft}-{512}-{win_length}/test.pt\")\n","\n","\n","for hop_length in [256, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/spectrograms/{2048}-{hop_length}-{2048}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/spectrograms/{2048}-{hop_length}-{2048}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/spectrograms/{2048}-{hop_length}-{2048}/test.pt\")\n"]},{"cell_type":"markdown","metadata":{"id":"bgYyIxHJh25O"},"source":["### Mel Spectrogram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejZ5v5tBh25O"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","\n","\n","for n_fft in [512, 1024, 2048, 4096]:\n","    for win_length in [512, 1024, 2048, 4096]:\n","        if n_fft < win_length:\n","            continue\n","        else:\n","            os.makedirs(f\"{FILEPATH}/{DIR}/mel_spectrograms/{n_fft}-{512}-{win_length}-{128}\", exist_ok=True)\n","            spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_train]]))\n","            spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_test]]))\n","            spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","            spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","            torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mel_spectrograms/{n_fft}-{512}-{win_length}-{128}/train.pt\")\n","            torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mel_spectrograms/{n_fft}-{512}-{win_length}-{128}/test.pt\")\n","\n","\n","for hop_length in [256, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{hop_length}-{2048}-{128}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{hop_length}-{2048}-{128}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{hop_length}-{2048}-{128}/test.pt\")\n","\n","\n","for mel_features in [64, 128, 192, 256]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{512}-{2048}-{mel_features}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mel_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{512}-{2048}-{mel_features}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mel_spectrograms/{2048}-{512}-{2048}-{mel_features}/test.pt\")"]},{"cell_type":"markdown","metadata":{"id":"_VOfBuf6h25P"},"source":["### MFCCs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1WeX2vLh25P"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","\n","for n_fft in [512, 1024, 2048, 4096]:\n","    for win_length in [512, 1024, 2048, 4096]:\n","        if n_fft < win_length:\n","            continue\n","        else:\n","            os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{n_fft}-{512}-{win_length}-{128}-{13}\", exist_ok=True)\n","            spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_train]]))\n","            spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, fft=n_fft, win=win_length) for sample, sr in waveforms_test]]))\n","            spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","            spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","            torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{n_fft}-{512}-{win_length}-{128}-{13}/train.pt\")\n","            torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{n_fft}-{512}-{win_length}-{128}-{13}/test.pt\")\n","\n","\n","for hop_length in [256, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{hop_length}-{2048}-{128}-{13}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, hop=hop_length, fft=2048, win=2048) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{hop_length}-{2048}-{128}-{13}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{hop_length}-{2048}-{128}-{13}/test.pt\")\n","\n","\n","for mel_features in [64, 128, 192, 256]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{mel_features}-{13}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mel=mel_features) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{mel_features}-{13}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{mel_features}-{13}/test.pt\")\n","\n","for mfcc_components in [5, 9, 13, 20]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{128}-{mfcc_components}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mfcc_bins=mfcc_components) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.mfcc_spectrogram(sample, sr, mfcc_bins=mfcc_components) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{128}-{mfcc_components}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/mfcc_spectrograms/{2048}-{512}-{2048}-{128}-{mfcc_components}/test.pt\")"]},{"cell_type":"markdown","metadata":{"id":"quC-qqvvh25P"},"source":["### CQT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYLOq4ruh25P"},"outputs":[],"source":["for hop_length in [256, 512, 1024, 2048]:\n","    os.makedirs(f\"{FILEPATH}/{DIR}/cqt_spectrograms/{hop_length}\", exist_ok=True)\n","    spectrogram_X_train = Tensor(np.array([x for x, _ in [spectrograms.cqt_spectrogram(sample, sr, hop=hop_length) for sample, sr in waveforms_train]]))\n","    spectrogram_X_test = Tensor(np.array([x for x, _ in [spectrograms.cqt_spectrogram(sample, sr, hop=hop_length) for sample, sr in waveforms_test]]))\n","    spectrogram_train = TensorDataset(spectrogram_X_train, torch.LongTensor(y_train))\n","    spectrogram_test = TensorDataset(spectrogram_X_test, torch.LongTensor(y_test))\n","    torch.save(spectrogram_train, f\"{FILEPATH}/{DIR}/cqt_spectrograms/{hop_length}/train.pt\")\n","    torch.save(spectrogram_test, f\"{FILEPATH}/{DIR}/cqt_spectrograms/{hop_length}/test.pt\")"]},{"cell_type":"markdown","metadata":{"id":"v-ac1vMrh25P"},"source":["# ANN Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEiy6I9ph25P"},"outputs":[],"source":["def train(model, train_dataset, test_dataset, num_epochs, device):\n","    criterion = nn.CrossEntropyLoss()\n","    #optimizer = optim.SGD(model.parameters(), lr=0.01)\n","    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","    batch_size = 32\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    test_results = df()\n","    train_results = df()\n","\n","    epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        model.train()\n","        for inputs, targets in train_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            running_loss += loss.item()\n","\n","        epoch_progress_bar.update(1)\n","\n","        # Print average loss for the epoch\n","        test_results = test_network(model, test_dataset, criterion, test_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'test')\n","        train_results = test_network(model, train_dataset, criterion, test_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'train')\n","\n","    del model\n","    del inputs\n","    del targets\n","    del optimizer\n","    del criterion\n","    del loss\n","    gc.collect()\n","    if device == 'cuda': torch.cuda.empty_cache()\n","    elif device == 'mps': torch.mps.empty_cache()\n","\n","    print(\"Training finished!\")\n","    return test_results, train_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ha95p49VjS-I"},"outputs":[],"source":["DIR = \"spectrogram_tensors\"\n","FILEPATH = \"/content\"\n","\n","X = dataset[dataset['supercategory']=='music']['filename'].tolist()\n","Y = dataset[dataset['supercategory']=='music']['category'].map(lambda x: LABEL_MAPPINGS[x]).tolist()\n","\n","label_encoder = LabelEncoder()\n","Y_encoded = label_encoder.fit_transform(Y)\n","\n","label_mappings = {encoded_label: original_label for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n","print(label_mappings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JoO1V9rh25P"},"outputs":[],"source":["for spectrogram_type in ['mfcc_spectrograms', 'cqt_spectrograms', 'mel_spectrograms', 'spectrograms']:\n","  for spectrogram_files in os.listdir(f\"{FILEPATH}/{DIR}/{spectrogram_type}\"):\n","    if not os.path.isfile(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/train.pt\"):\n","      print(f\"{spectrogram_type}/{spectrogram_files}\")\n","      train_dataset = torch.load(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/train.pt\")\n","      test_dataset = torch.load(f\"{FILEPATH}/{DIR}/{spectrogram_type}/{spectrogram_files}/test.pt\")\n","\n","      x_shape = train_dataset[0][0].shape\n","      scale_factor = min(30000/(x_shape[0] * x_shape[1]), 1)\n","\n","      transform = transforms.Compose([\n","          transforms.ToTensor(),\n","          transforms.Resize(tuple(int(dim * scale_factor) for dim in x_shape), antialias=True)\n","      ])\n","\n","      train_dataset = [(transform(sample.numpy()), target) for sample, target in train_dataset]\n","      test_dataset = [(transform(sample.numpy()), target) for sample, target in test_dataset]\n","\n","      flattened_x_shape = int(x_shape[0]* scale_factor) * int(x_shape[1] * scale_factor)\n","\n","\n","      model = Triangle_Network(flattened_x_shape, len(label_encoder.classes_), beta=False).to(device)\n","      num_epochs = 120\n","\n","      criterion = nn.CrossEntropyLoss()\n","      #optimizer = optim.SGD(model.parameters(), lr=0.01)\n","      optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","      batch_size = 32\n","      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","      test_results = df()\n","      train_results = df()\n","\n","      epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","      for epoch in range(num_epochs):\n","          running_loss = 0.0\n","          model.train()\n","          for inputs, targets in train_loader:\n","              inputs, targets = inputs.to(device), targets.to(device)\n","\n","              outputs = model(inputs)\n","              loss = criterion(outputs, targets)\n","\n","              loss.backward()\n","              optimizer.step()\n","              optimizer.zero_grad()\n","\n","              running_loss += loss.item()\n","\n","          epoch_progress_bar.update(1)\n","\n","          # Print average loss for the epoch\n","          test_results = test_network(model, test_dataset, criterion, test_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'test')\n","          train_results = test_network(model, train_dataset, criterion, train_results, epoch, device, printable=(epoch_progress_bar if ((epoch+1) % 15 == 0) else None), test_train = 'train')\n","\n","      del model\n","      del inputs\n","      del targets\n","      del optimizer\n","      del criterion\n","      del loss\n","      gc.collect()\n","      if device == 'cuda': torch.cuda.empty_cache()\n","      elif device == 'mps': torch.mps.empty_cache()\n","\n","      test_results.to_csv(f\"/content/drive/MyDrive/spectrogram_tensors/{spectrogram_type}/{spectrogram_files}/test.csv\")\n","      train_results.to_csv(f\"/content/drive/MyDrive/spectrogram_tensors/{spectrogram_type}/{spectrogram_files}/train.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLIP4saookhS"},"outputs":[],"source":["%reset"]},{"cell_type":"markdown","metadata":{},"source":["# Input Encodings"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["from snntorch import functional as SF\n","\n","FILEPATH = \"../\"\n","TEST_TYPE = \"IST non-JNB results/input_encoding\"\n","SPECTROGRAMS = ['mel_spectrograms']"]},{"cell_type":"markdown","metadata":{},"source":["## Direct Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ENCODING_TYPE = \"direct_encoding\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\"):\n","        print(f\"{spectrogram_type}\")\n","        train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")\n","        test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","        num_epochs = 100\n","\n","        criterion = criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        batch_size = 125\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","        train_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","    \n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            acc = 0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(1)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                to_print = (epoch_progress_bar if ((epoch+1) % 15 == 0) else None)\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.csv\")\n","        train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Time Contrast Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from delta import delta"]},{"cell_type":"markdown","metadata":{},"source":["## Direct Time Contrast"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"direct_TC_encoding\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in ['mfcc_spectrograms', 'mel_spectrograms']:\n","    original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","    original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","    direct_TC_test = delta(original_test_dataset, padding = True)\n","    direct_TC_train = delta(original_train_dataset, padding = True)\n","\n","    torch.save(direct_TC_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","    torch.save(direct_TC_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\"):\n","        print(f\"{spectrogram_type}\")\n","        train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")\n","        test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","        num_epochs = 100\n","\n","        criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        batch_size = 125\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","        train_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","    \n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            acc = 0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                # inputs in form of (time, batch, features)\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(0)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                to_print = (epoch_progress_bar if ((epoch+1) % 15 == 0) else None)\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.csv\")\n","        train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Time Contrast or Threshold Based"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from delta import delta\n","\n","NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"time_contrast\"\n","\n","THRESHOLDS = [0.01, 0.025, 0.05, 0.10, 0.20, 0.50]\n","OFF_SPIKES = [True, False]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","            original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","            TC_test = delta(original_test_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True)\n","            TC_train = delta(original_train_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True)\n","\n","            os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}\", exist_ok=True)\n","\n","            torch.save(TC_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","            torch.save(TC_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\"):\n","                print(f\"{threshold}_{off_spike}_{spectrogram_type}\")\n","                train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")\n","                test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","\n","                # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","                x_shape = train_dataset[0][0].shape\n","\n","                # Assuming the shape is t x f\n","                features_shape = x_shape[1]\n","                POP_ENCODING = 10\n","                classes = len(label_encoder.classes_)\n","                output_shape = classes * POP_ENCODING\n","\n","\n","                model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","                num_epochs = 100\n","\n","                criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","                optimizer = optim.Adam(model.parameters(), lr=0.005)\n","\n","                batch_size = 120\n","                train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","                test_results = df()\n","                train_results = df()\n","\n","                epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","                for epoch in range(num_epochs):\n","                    running_loss = 0.0\n","                    total = 0\n","                    model.train()\n","                    for inputs, targets in train_loader:\n","                        # inputs in form of (time, batch, features)\n","                        inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                        spikes, _ = model(inputs)\n","\n","                        loss = criterion(spikes, targets)\n","\n","                        loss.backward()\n","                        optimizer.step()\n","                        optimizer.zero_grad()\n","\n","                        running_loss += loss.item()\n","                        total += spikes.size(0)\n","\n","                    epoch_progress_bar.update(1)\n","\n","                    print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                    \n","                    # Print average loss for the epoch\n","                    if ((epoch+1) % 5 == 0):\n","                        test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                        train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","                del model\n","                del inputs\n","                del targets\n","                del optimizer\n","                del criterion\n","                del loss\n","                gc.collect()\n","                if device == 'cuda': torch.cuda.empty_cache()\n","                elif device == 'mps': torch.mps.empty_cache()\n","\n","                test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.csv\")\n","                train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Cumulative Time Contrast (SF)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from delta import delta\n","\n","NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"step_forward\"\n","\n","THRESHOLDS = [0.01, 0.025, 0.05, 0.10, 0.20, 0.50]\n","OFF_SPIKES = [True, False]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","            original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","            TC_test = delta(original_test_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True, cumulative=True)\n","            TC_train = delta(original_train_dataset, padding = True, threshold=threshold, off_spike=off_spike, threshold_as_percentage=True, cumulative=True)\n","\n","            os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}\", exist_ok=True)\n","\n","            torch.save(TC_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","            torch.save(TC_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for threshold in THRESHOLDS:\n","        for off_spike in OFF_SPIKES:\n","            if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\"):\n","                print(f\"{threshold}_{off_spike}_{spectrogram_type}\")\n","                train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.pt\")\n","                test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.pt\")\n","\n","                # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","                x_shape = train_dataset[0][0].shape\n","\n","                # Assuming the shape is t x f\n","                features_shape = x_shape[1]\n","                POP_ENCODING = 10\n","                classes = len(label_encoder.classes_)\n","                output_shape = classes * POP_ENCODING\n","\n","\n","                model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","                num_epochs = 100\n","\n","                criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","                optimizer = optim.Adam(model.parameters(), lr=0.005)\n","\n","                batch_size = 120\n","                train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","                test_results = df()\n","                train_results = df()\n","\n","                epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","                for epoch in range(num_epochs):\n","                    running_loss = 0.0\n","                    total = 0\n","                    model.train()\n","                    for inputs, targets in train_loader:\n","                        # inputs in form of (time, batch, features)\n","                        inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                        spikes, _ = model(inputs)\n","\n","                        loss = criterion(spikes, targets)\n","\n","                        loss.backward()\n","                        optimizer.step()\n","                        optimizer.zero_grad()\n","\n","                        running_loss += loss.item()\n","                        total += spikes.size(0)\n","\n","                    epoch_progress_bar.update(1)\n","\n","                    print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                    \n","                    # Print average loss for the epoch\n","                    if ((epoch+1) % 5 == 0):\n","                        test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                        train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","                del model\n","                del inputs\n","                del targets\n","                del optimizer\n","                del criterion\n","                del loss\n","                gc.collect()\n","                if device == 'cuda': torch.cuda.empty_cache()\n","                elif device == 'mps': torch.mps.empty_cache()\n","\n","                test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/test.csv\")\n","                train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{threshold}_{off_spike}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Rate Encoding Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from rate import rate, count"]},{"cell_type":"markdown","metadata":{},"source":["## Count Encoding (Whole Spectrogram)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"count_encoding\"\n","SUB = \"whole\"\n","\n","for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","        original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","        poisson_test = count(original_test_dataset, max_spikes=n_count)\n","        poisson_train = count(original_train_dataset, max_spikes=n_count)\n","\n","        os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}\", exist_ok=True)\n","\n","        torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","        torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\"):\n","            print(f\"{spectrogram_type}\")\n","            train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")\n","            test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","\n","            x_shape = train_dataset[0][0].shape\n","            scale_factor = min(30000/(x_shape[0] * x_shape[1]), 1)\n","\n","            transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(tuple(int(dim * scale_factor) for dim in x_shape), antialias=True)\n","            ])\n","\n","            train_dataset = [(transform(sample.numpy()), target) for sample, target in train_dataset]\n","            test_dataset = [(transform(sample.numpy()), target) for sample, target in test_dataset]\n","\n","            flattened_x_shape = int(x_shape[0]* scale_factor) * int(x_shape[1] * scale_factor)\n","\n","            features_shape = flattened_x_shape\n","            POP_ENCODING = 10\n","            classes = len(label_encoder.classes_)\n","            output_shape = classes * POP_ENCODING\n","\n","\n","            model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","            num_epochs = 100\n","\n","            criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","            optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","            batch_size = 120\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","            test_results = df()\n","            train_results = df()\n","\n","            epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","            for epoch in range(num_epochs):\n","                running_loss = 0.0\n","                total = 0\n","                model.train()\n","                for inputs, targets in train_loader:\n","                    # inputs in form of (time, batch, features)\n","                    inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                    spikes, _ = model(inputs)\n","\n","                    loss = criterion(spikes, targets)\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    running_loss += loss.item()\n","                    total += spikes.size(0)\n","\n","                epoch_progress_bar.update(1)\n","\n","                print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                \n","                # Print average loss for the epoch\n","                if ((epoch+1) % 5 == 0):\n","                    test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                    train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","            del model\n","            del inputs\n","            del targets\n","            del optimizer\n","            del criterion\n","            del loss\n","            gc.collect()\n","            if device == 'cuda': torch.cuda.empty_cache()\n","            elif device == 'mps': torch.mps.empty_cache()\n","\n","            test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.csv\")\n","            train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Count Encoding (Extending Dims)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"count_encoding\"\n","SUB = \"extend\"\n","\n","for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","        original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","        poisson_test = count(original_test_dataset, max_spikes=n_count, time_varying=True)\n","        poisson_train = count(original_train_dataset, max_spikes=n_count, time_varying=True)\n","\n","        os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}\", exist_ok=True)\n","\n","        torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","        torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\"):\n","            print(f\"{spectrogram_type}\")\n","            train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")\n","            test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","\n","            # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","            x_shape = train_dataset[0][0].shape\n","\n","            # Assuming the shape is t x f\n","            features_shape = x_shape[1]\n","            POP_ENCODING = 10\n","            classes = len(label_encoder.classes_)\n","            output_shape = classes * POP_ENCODING\n","\n","\n","            model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","            num_epochs = 120\n","\n","            criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","            optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","            batch_size = 120\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","            test_results = df()\n","            train_results = df()\n","\n","            epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","            for epoch in range(num_epochs):\n","                running_loss = 0.0\n","                total = 0\n","                model.train()\n","                for inputs, targets in train_loader:\n","                    # inputs in form of (time, batch, features)\n","                    inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                    spikes, _ = model(inputs)\n","\n","                    loss = criterion(spikes, targets)\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    running_loss += loss.item()\n","                    total += spikes.size(0)\n","\n","                epoch_progress_bar.update(1)\n","\n","                print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                \n","                # Print average loss for the epoch\n","                if ((epoch+1) % 5 == 0):\n","                    test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                    train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","            del model\n","            del inputs\n","            del targets\n","            del optimizer\n","            del criterion\n","            del loss\n","            gc.collect()\n","            if device == 'cuda': torch.cuda.empty_cache()\n","            elif device == 'mps': torch.mps.empty_cache()\n","\n","            test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.csv\")\n","            train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Poisson Encoding (Whole Spectrogram)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"poisson_count\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","        original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","        poisson_test = rate(original_test_dataset, extend=False, num_steps=n_count)\n","        poisson_train = rate(original_train_dataset, extend=False, num_steps=n_count)\n","\n","        os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}\", exist_ok=True)\n","\n","        torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","        torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    for n_count in [5,7,10,15]:\n","        if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\"):\n","            print(f\"{spectrogram_type}\")\n","            train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.pt\")\n","            test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.pt\")\n","\n","            x_shape = train_dataset[0][0].shape\n","            scale_factor = min(30000/(x_shape[0] * x_shape[1]), 1)\n","\n","            transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(tuple(int(dim * scale_factor) for dim in x_shape), antialias=True)\n","            ])\n","\n","            train_dataset = [(transform(sample.numpy()), target) for sample, target in train_dataset]\n","            test_dataset = [(transform(sample.numpy()), target) for sample, target in test_dataset]\n","\n","            flattened_x_shape = int(x_shape[0]* scale_factor) * int(x_shape[1] * scale_factor)\n","\n","            features_shape = flattened_x_shape\n","            POP_ENCODING = 10\n","            classes = len(label_encoder.classes_)\n","            output_shape = classes * POP_ENCODING\n","\n","\n","            model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","            num_epochs = 100\n","\n","            criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","            optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","            batch_size = 120\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","            test_results = df()\n","            train_results = df()\n","\n","            epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","            for epoch in range(num_epochs):\n","                running_loss = 0.0\n","                total = 0\n","                model.train()\n","                for inputs, targets in train_loader:\n","                    # inputs in form of (time, batch, features)\n","                    inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                    spikes, _ = model(inputs)\n","\n","                    loss = criterion(spikes, targets)\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    running_loss += loss.item()\n","                    total += spikes.size(0)\n","\n","                epoch_progress_bar.update(1)\n","\n","                print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","                \n","                # Print average loss for the epoch\n","                if ((epoch+1) % 5 == 0):\n","                    test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                    train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","            del model\n","            del inputs\n","            del targets\n","            del optimizer\n","            del criterion\n","            del loss\n","            gc.collect()\n","            if device == 'cuda': torch.cuda.empty_cache()\n","            elif device == 'mps': torch.mps.empty_cache()\n","\n","            test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/test.csv\")\n","            train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{SUB}/{n_count}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Poisson Encoding (Keeping Dimensions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NON_ENCODED = \"direct_encoding\"\n","ENCODING_TYPE = \"poisson_dims\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    original_test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/test.pt\")\n","    original_train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{NON_ENCODED}/{spectrogram_type}/train.pt\")\n","\n","    poisson_test = rate(original_test_dataset)\n","    poisson_train = rate(original_train_dataset)\n","\n","    os.makedirs(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}\", exist_ok=True)\n","\n","    torch.save(poisson_test, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","    torch.save(poisson_train, f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for spectrogram_type in SPECTROGRAMS:\n","    if not os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\") and os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\"):\n","        print(f\"{spectrogram_type}\")\n","        train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.pt\")\n","        test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","        num_epochs = 100\n","\n","        criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.005)\n","\n","        batch_size = 100\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","        test_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                # inputs in form of (time, batch, features)\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(0)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        test_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/test.csv\")\n","        train_results.to_csv(f\"{FILEPATH}/{TEST_TYPE}/{ENCODING_TYPE}/{spectrogram_type}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Poisson Encoding (Extending Dims)"]},{"cell_type":"markdown","metadata":{},"source":["### Note that this took too long for Count Encoding (Extending Dims) --- i.e. it takes upwards of 50hrs to train Count_n=15"]},{"cell_type":"markdown","metadata":{},"source":["## Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from snntorch import utils\n","\n","FILEPATH = \"../\"\n","TEST_TYPE = \"IST non-JNB results/input_encoding\"\n","DATA_ENCODING = \"direct_encoding\"\n","ENCODING_TYPE = \"autoencoder\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AutoEncoder(nn.Module):\n","    def __init__(self, features_shape, coding_size):\n","        super().__init__()\n","\n","        # Encoder and Decoder is not time dependent within themselves. Time is handled by the Autoencoder\n","\n","        # Encoder\n","        self.encoder = Triangle_Network(features_shape, coding_size, beta=0.9, time_dependent=False)\n","\n","        # Decoder\n","        self.decoder = Triangle_Network(coding_size, features_shape, beta=0.9, time_dependent=False)\n","\n","    def forward(self, x):\n","        utils.reset(self.encoder)\n","        utils.reset(self.decoder)\n","\n","        # Assume the data is in the form [time x batch x features]\n","        num_steps = x.size(0)\n","\n","        spk_mem2=[]\n","        spk_rec=[]\n","        \n","        # Over the time dimension\n","        for step in range(num_steps):\n","            # Encode\n","            spk_x, _ = self.encoder(x[step])\n","            spk_rec.append(spk_x)\n","            \n","            # Decode\n","            _, x_mem_recon = self.decoder(spk_x)\n","            spk_mem2.append(x_mem_recon)\n","\n","\n","        # Same Dimensions as input: [time x batch x features]\n","        spk_rec = torch.stack(spk_rec,dim=0)\n","        spk_mem2 = torch.stack(spk_mem2,dim=0)\n","\n","        return spk_mem2\n","    \n","    def encode(self,x):\n","        spk_x, mem_x = self.encoder(x)\n","        return spk_x, mem_x\n","\n","    def decode(self,x):\n","        spk_x, mem_x = self.decoder(x)\n","        return spk_x, mem_x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TOLERANCE = 0.05 #dB\n","\n","def check_accuracy(model, test_dataset: TensorDataset, train_dataset: TensorDataset):\n","    model.eval()\n","    test_x, _ = test_dataset.tensors\n","    train_x, _ = train_dataset.tensors\n","    data = torch.cat((test_x, train_x), dim=0).to(device)\n","\n","    mem = model(data).detach().cpu().numpy()\n","\n","    # Check predictions against tolerance\n","    within_tolerance = np.abs(data.detach().cpu().numpy() - mem) <= TOLERANCE\n","\n","    # Calculate accuracy\n","    accuracy = np.mean(within_tolerance) * 100\n","\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set upper and lower bound percentages of the encoded layer\n","upper_bound = 1.0\n","lower_bound = 0.75\n","MAX_MODELS = 6\n","\n","for spectrogram_type in SPECTROGRAMS:\n","    i = 0\n","    while i <= MAX_MODELS:\n","        mid_bound = (upper_bound + lower_bound) / 2\n","        if os.path.isfile(f\"{FILEPATH}/{TEST_TYPE}/{DATA_ENCODING}/{spectrogram_type}/train.pt\") and not os.path.isfile(f\"{FILEPATH}{TEST_TYPE}/autoencoder/{mid_bound}/{spectrogram_type}/model.pt\"):\n","            print(f\"{spectrogram_type} {mid_bound}\")\n","            train_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{DATA_ENCODING}/{spectrogram_type}/train.pt\")\n","            test_dataset = torch.load(f\"{FILEPATH}/{TEST_TYPE}/{DATA_ENCODING}/{spectrogram_type}/test.pt\")\n","\n","            # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","            x_shape = train_dataset[0][0].shape\n","\n","            # Assuming the shape is t x f\n","            features_shape = x_shape[1]\n","\n","            model = AutoEncoder(features_shape, round(features_shape * mid_bound)).to(device)\n","            num_epochs = 75\n","\n","            criterion = nn.L1Loss()\n","\n","            optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","            # Higher batch size for quicker training and a more general model\n","            batch_size = 175\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=3, shuffle=True)\n","\n","            results = df()\n","\n","            epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","            for epoch in range(num_epochs):\n","                running_loss = 0.0\n","                total = 0\n","                model.train()\n","                for inputs, _ in train_loader:\n","                    # inputs in form of (time, batch, features)\n","                    inputs= inputs.transpose(0, 1).to(device)\n","\n","                    mem = model(inputs)\n","\n","                    loss = criterion(mem, inputs)\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","\n","                    running_loss += loss.item()\n","                    total += mem.size(0)\n","\n","\n","                epoch_progress_bar.update(1)\n","                \n","\n","                accuracy = check_accuracy(model, test_dataset, train_dataset)\n","                epoch_progress_bar.set_description(f\"Epoch {epoch+1} Running loss: {running_loss/total} Accuracy (within 5dB): {accuracy}\")\n","                results = results._append({'Epoch': epoch+1, 'Loss': running_loss/total, 'Accuracy (within 5dB)': accuracy}, ignore_index=True)\n","\n","\n","                # Decrease upper bound on success -- save models\n","                if accuracy >= 90:\n","                    success = True\n","                    if accuracy == 100:\n","                        i = MAX_MODELS\n","                        print(\"100% Accuracy -- breaking\")\n","                    print(\"Within 90% -- trying towards lower bound\")\n","\n","                    upper_bound = mid_bound; i += 1\n","\n","                    os.makedirs(f\"{FILEPATH}{TEST_TYPE}/autoencoder/{mid_bound}/{spectrogram_type}\", exist_ok=True)\n","                    torch.save(model, f\"{FILEPATH}{TEST_TYPE}/autoencoder/{mid_bound}/{spectrogram_type}/model.pt\")\n","                    results.to_csv(f\"{FILEPATH}{TEST_TYPE}/autoencoder/{mid_bound}/{spectrogram_type}/model.csv\")\n","                    break\n","            \n","            # Raise lower bound on fail\n","            os.makedirs(f\"{FILEPATH}{TEST_TYPE}/autoencoder/{mid_bound}/{spectrogram_type}\", exist_ok=True)\n","            torch.save(model, f\"{FILEPATH}{TEST_TYPE}/autoencoder/{mid_bound}/{spectrogram_type}/model.pt\")\n","            results.to_csv(f\"{FILEPATH}{TEST_TYPE}/autoencoder/{mid_bound}/{spectrogram_type}/model.csv\")\n","            lower_bound = mid_bound; i += 1\n","                \n","\n","            del model\n","            del inputs\n","            del optimizer\n","            del criterion\n","            del loss\n","            gc.collect()\n","            if device == 'cuda': torch.cuda.empty_cache()\n","            elif device == 'mps': torch.mps.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["# Architectures & Action Potentials"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["FILEPATH = \"../\"\n","TEST_TYPE = \"IST non-JNB results/architectures\"\n","ENCODING = \"time_contrast/0.1_True/mel_spectrograms\""]},{"cell_type":"markdown","metadata":{},"source":["## MLP (Triangle)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Ignore beta = 0.90 as this has been worked out before depending on the input dataset\n","BETAS = [0.70, 0.80, 0.95]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0601d53c98247b4b87acf74aa1b1041","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.007021049912364338\n","Epoch 2 Running loss: 0.007013114115681511\n","Epoch 3 Running loss: 0.012394037109594375\n","Epoch 4 Running loss: 0.007441623332782294\n","Epoch 5 Running loss: 0.00843354688284877\n","Epoch 6 Running loss: 0.0112912898627333\n","Epoch 7 Running loss: 0.013138120738081276\n","Epoch 8 Running loss: 0.009623245880626642\n","Epoch 9 Running loss: 0.00693739203218454\n","Epoch 10 Running loss: 0.008786819613398835\n","Epoch 11 Running loss: 0.007384962833727511\n","Epoch 12 Running loss: 0.006104516502196035\n","Epoch 13 Running loss: 0.007848757524459888\n","Epoch 14 Running loss: 0.006086957441589322\n","Epoch 15 Running loss: 0.00724333391402857\n","Epoch 16 Running loss: 0.010026747807146262\n","Epoch 17 Running loss: 0.006076164615040008\n","Epoch 18 Running loss: 0.008242853533345671\n","Epoch 19 Running loss: 0.0075491525875493746\n","Epoch 20 Running loss: 0.007547269899624225\n","Epoch 21 Running loss: 0.007000677025737092\n","Epoch 22 Running loss: 0.007970677777981987\n","Epoch 23 Running loss: 0.007200478364865239\n","Epoch 24 Running loss: 0.00599268445382103\n","Epoch 25 Running loss: 0.008084744119796509\n","Epoch 26 Running loss: 0.007357468429845743\n","Epoch 27 Running loss: 0.006858275721248346\n","Epoch 28 Running loss: 0.006666746288061904\n","Epoch 29 Running loss: 0.006413896815083659\n","Epoch 30 Running loss: 0.008422737685255349\n","Epoch 31 Running loss: 0.008778917522856984\n","Epoch 32 Running loss: 0.007933401832946193\n","Epoch 33 Running loss: 0.007381020643459722\n","Epoch 34 Running loss: 0.00779648996389712\n","Epoch 35 Running loss: 0.00682677514255999\n","Epoch 36 Running loss: 0.008126132023601105\n","Epoch 37 Running loss: 0.006105308358471234\n","Epoch 38 Running loss: 0.007276721846181364\n","Epoch 39 Running loss: 0.007805007524764576\n","Epoch 40 Running loss: 0.006449996949003908\n","Epoch 41 Running loss: 0.005220602409313091\n","Epoch 42 Running loss: 0.007285024744634049\n","Epoch 43 Running loss: 0.007641048857960076\n","Epoch 44 Running loss: 0.005592285015712531\n","Epoch 45 Running loss: 0.006400627545274484\n","Epoch 46 Running loss: 0.005669033565460302\n","Epoch 47 Running loss: 0.010093430551096273\n","Epoch 48 Running loss: 0.006234018852154668\n","Epoch 49 Running loss: 0.0053081229662362\n","Epoch 50 Running loss: 0.007603574675112106\n","Epoch 51 Running loss: 0.007709014053923634\n","Epoch 52 Running loss: 0.006332389653300325\n","Epoch 53 Running loss: 0.00709767634876239\n","Epoch 54 Running loss: 0.006355731251140753\n","Epoch 55 Running loss: 0.004821554086685847\n","Epoch 56 Running loss: 0.004871278644179384\n","Epoch 57 Running loss: 0.007508005577916154\n","Epoch 58 Running loss: 0.006638041986063266\n","Epoch 59 Running loss: 0.008993076916319875\n","Epoch 60 Running loss: 0.007098633403214403\n","Epoch 61 Running loss: 0.006635215240545547\n","Epoch 62 Running loss: 0.0076178481784491495\n","Epoch 63 Running loss: 0.010632334711452642\n","Epoch 64 Running loss: 0.004894343047094328\n","Epoch 65 Running loss: 0.0070708493074289145\n","Epoch 66 Running loss: 0.005586643140917769\n","Epoch 67 Running loss: 0.006924550182903156\n","Epoch 68 Running loss: 0.005056107196564111\n","Epoch 69 Running loss: 0.006903424049718692\n","Epoch 70 Running loss: 0.005818697400748158\n","Epoch 71 Running loss: 0.007129784494924088\n","Epoch 72 Running loss: 0.005437394705252906\n","Epoch 73 Running loss: 0.006538704751779477\n","Epoch 74 Running loss: 0.0052492758526969645\n","Epoch 75 Running loss: 0.004980550953945793\n","Epoch 76 Running loss: 0.006306357562732392\n","Epoch 77 Running loss: 0.005961201156671055\n","Epoch 78 Running loss: 0.007676595220931422\n","Epoch 79 Running loss: 0.006204694890366576\n","Epoch 80 Running loss: 0.007015166667322762\n","Epoch 81 Running loss: 0.005513570345819187\n","Epoch 82 Running loss: 0.00722654977926431\n","Epoch 83 Running loss: 0.009920939374655581\n","Epoch 84 Running loss: 0.006837051992599195\n","Epoch 85 Running loss: 0.007243240841280538\n","Epoch 86 Running loss: 0.005088444858694229\n","Epoch 87 Running loss: 0.007412997392800669\n","Epoch 88 Running loss: 0.013852954672548337\n","Epoch 89 Running loss: 0.010318986524027376\n","Epoch 90 Running loss: 0.007302210353814756\n","Epoch 91 Running loss: 0.006475549917251538\n","Epoch 92 Running loss: 0.007473176374983864\n","Epoch 93 Running loss: 0.006558754097539396\n","Epoch 94 Running loss: 0.005139970431883876\n","Epoch 95 Running loss: 0.00939857674102052\n","Epoch 96 Running loss: 0.0068450514881755594\n","Epoch 97 Running loss: 0.004947471066404837\n","Epoch 98 Running loss: 0.006416494187455588\n","Epoch 99 Running loss: 0.00535059181568674\n","Epoch 100 Running loss: 0.0053274848114568205\n","0.8\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e395029e4c34146b3cfc528dffe2f9c","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.007011226762217074\n","Epoch 2 Running loss: 0.007021049912364338\n","Epoch 3 Running loss: 0.005425203555879501\n","Epoch 4 Running loss: 0.006485328744775571\n","Epoch 5 Running loss: 0.010992228984832764\n","Epoch 6 Running loss: 0.0059967677528484946\n","Epoch 7 Running loss: 0.006848691465755621\n","Epoch 8 Running loss: 0.00810097010371784\n","Epoch 9 Running loss: 0.006059685977883994\n","Epoch 10 Running loss: 0.006579677279764852\n","Epoch 11 Running loss: 0.007439252096243179\n","Epoch 12 Running loss: 0.008244912845258134\n","Epoch 13 Running loss: 0.00905303299998323\n","Epoch 14 Running loss: 0.008406423532162992\n","Epoch 15 Running loss: 0.007822036362303712\n","Epoch 16 Running loss: 0.010311042538847024\n","Epoch 17 Running loss: 0.009344122280328038\n","Epoch 18 Running loss: 0.008107487386027084\n","Epoch 19 Running loss: 0.006970353019884981\n","Epoch 20 Running loss: 0.007190502584932711\n","Epoch 21 Running loss: 0.005444800891812559\n","Epoch 22 Running loss: 0.007085426166034735\n","Epoch 23 Running loss: 0.00957926279439713\n","Epoch 24 Running loss: 0.009269364535237273\n","Epoch 25 Running loss: 0.010214963279212244\n","Epoch 26 Running loss: 0.009072067448125479\n","Epoch 27 Running loss: 0.0071303905389560295\n","Epoch 28 Running loss: 0.007401532448899632\n","Epoch 29 Running loss: 0.009016064790110238\n","Epoch 30 Running loss: 0.007345497227324464\n","Epoch 31 Running loss: 0.006892179528745219\n","Epoch 32 Running loss: 0.007085264871676509\n","Epoch 33 Running loss: 0.006083382037691415\n","Epoch 34 Running loss: 0.00864030892094865\n","Epoch 35 Running loss: 0.006091250112643257\n","Epoch 36 Running loss: 0.0058511410848782084\n","Epoch 37 Running loss: 0.005978877647235371\n","Epoch 38 Running loss: 0.006356707967508334\n","Epoch 39 Running loss: 0.006467589840721399\n","Epoch 40 Running loss: 0.009095185671370631\n","Epoch 41 Running loss: 0.007527458591583057\n","Epoch 42 Running loss: 0.006658634438682288\n","Epoch 43 Running loss: 0.010923437226694612\n","Epoch 44 Running loss: 0.007228517399047511\n","Epoch 45 Running loss: 0.006587589229531467\n","Epoch 46 Running loss: 0.00986330273052374\n","Epoch 47 Running loss: 0.006508817687963906\n","Epoch 48 Running loss: 0.007264114892520844\n","Epoch 49 Running loss: 0.006515131209985898\n","Epoch 50 Running loss: 0.0066704019761314026\n","Epoch 51 Running loss: 0.006743409287053556\n","Epoch 52 Running loss: 0.0062243312882920045\n","Epoch 53 Running loss: 0.006180007427264326\n","Epoch 54 Running loss: 0.00775379856554464\n","Epoch 55 Running loss: 0.0059865107551550336\n","Epoch 56 Running loss: 0.008170746195430572\n","Epoch 57 Running loss: 0.01095646962571068\n","Epoch 58 Running loss: 0.005594388769267085\n","Epoch 59 Running loss: 0.006182668355707162\n","Epoch 60 Running loss: 0.006336089616385512\n","Epoch 61 Running loss: 0.0053004848119168994\n","Epoch 62 Running loss: 0.005916458634903637\n","Epoch 63 Running loss: 0.005294145558017512\n","Epoch 64 Running loss: 0.004945553693780527\n","Epoch 65 Running loss: 0.006450196710257485\n","Epoch 66 Running loss: 0.005968457593704565\n","Epoch 67 Running loss: 0.005730157271741678\n","Epoch 68 Running loss: 0.006370788184217752\n","Epoch 69 Running loss: 0.007154222875357436\n","Epoch 70 Running loss: 0.005066967691285922\n","Epoch 71 Running loss: 0.007304193112796869\n","Epoch 72 Running loss: 0.006400811786468798\n","Epoch 73 Running loss: 0.006793793588400649\n","Epoch 74 Running loss: 0.006664458650369614\n","Epoch 75 Running loss: 0.007348457750040121\n","Epoch 76 Running loss: 0.005167676409641013\n","Epoch 77 Running loss: 0.0068156962006236796\n","Epoch 78 Running loss: 0.005836063394912135\n","Epoch 79 Running loss: 0.0053937301372948545\n","Epoch 80 Running loss: 0.008829281257745175\n","Epoch 81 Running loss: 0.006440647684347134\n","Epoch 82 Running loss: 0.0065106507688284685\n","Epoch 83 Running loss: 0.007443092501582429\n","Epoch 84 Running loss: 0.005431123720571256\n","Epoch 85 Running loss: 0.004699681632625409\n","Epoch 86 Running loss: 0.005755073346269016\n","Epoch 87 Running loss: 0.005668149207727597\n","Epoch 88 Running loss: 0.004923528328109473\n","Epoch 89 Running loss: 0.004903666151407808\n","Epoch 90 Running loss: 0.004923748322569144\n","Epoch 91 Running loss: 0.0077343161304157\n","Epoch 92 Running loss: 0.004395874331708248\n","Epoch 93 Running loss: 0.004839972042428038\n","Epoch 94 Running loss: 0.005840103847150223\n","Epoch 95 Running loss: 0.0063480280649166896\n","Epoch 96 Running loss: 0.006824666890092551\n","Epoch 97 Running loss: 0.005085225398548114\n","Epoch 98 Running loss: 0.006498879232345678\n","Epoch 99 Running loss: 0.004904774191697089\n","Epoch 100 Running loss: 0.008519175048834218\n","0.95\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b0198d64b25466a810ad206fb8d24c3","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.007019886955285605\n","Epoch 2 Running loss: 0.007050969539739834\n","Epoch 3 Running loss: 0.006777491956092298\n","Epoch 4 Running loss: 0.008217759025744356\n","Epoch 5 Running loss: 0.007097385466670076\n","Epoch 6 Running loss: 0.008619977643314642\n","Epoch 7 Running loss: 0.009975448584023374\n","Epoch 8 Running loss: 0.00716622207111444\n","Epoch 9 Running loss: 0.006663031471423067\n","Epoch 10 Running loss: 0.007140492716917215\n","Epoch 11 Running loss: 0.010818082303665698\n","Epoch 12 Running loss: 0.007207181507025283\n","Epoch 13 Running loss: 0.006856411028974734\n","Epoch 14 Running loss: 0.009799541566318598\n","Epoch 15 Running loss: 0.006726167358148593\n","Epoch 16 Running loss: 0.009938862377081435\n","Epoch 17 Running loss: 0.009293989061166684\n","Epoch 18 Running loss: 0.008795679948581294\n","Epoch 19 Running loss: 0.007553611319666854\n","Epoch 20 Running loss: 0.00748592634170581\n","Epoch 21 Running loss: 0.010359351627362042\n","Epoch 22 Running loss: 0.007813430060974706\n","Epoch 23 Running loss: 0.006949960423734622\n","Epoch 24 Running loss: 0.007203157907857681\n","Epoch 25 Running loss: 0.006367376818062779\n","Epoch 26 Running loss: 0.007037035954265168\n","Epoch 27 Running loss: 0.008519004518612506\n","Epoch 28 Running loss: 0.006498311083918563\n","Epoch 29 Running loss: 0.006183303392733248\n","Epoch 30 Running loss: 0.006253343230238357\n","Epoch 31 Running loss: 0.009135792049737022\n","Epoch 32 Running loss: 0.007051715645165489\n","Epoch 33 Running loss: 0.008510317665319473\n","Epoch 34 Running loss: 0.006337216963021519\n","Epoch 35 Running loss: 0.006451833695649339\n","Epoch 36 Running loss: 0.005657564687985963\n","Epoch 37 Running loss: 0.00717127199371021\n","Epoch 38 Running loss: 0.005870396336808372\n","Epoch 39 Running loss: 0.0055113355108438586\n","Epoch 40 Running loss: 0.0073307443160218555\n","Epoch 41 Running loss: 0.006573787893350132\n","Epoch 42 Running loss: 0.0054649749931435995\n","Epoch 43 Running loss: 0.006344848357069607\n","Epoch 44 Running loss: 0.008823425244218626\n","Epoch 45 Running loss: 0.004682201929330921\n","Epoch 46 Running loss: 0.0054224629800159714\n","Epoch 47 Running loss: 0.00787849443408247\n","Epoch 48 Running loss: 0.004960598037265741\n","Epoch 49 Running loss: 0.004914911434101982\n","Epoch 50 Running loss: 0.007475382984636691\n","Epoch 51 Running loss: 0.005580444305468672\n","Epoch 52 Running loss: 0.005724800473894364\n","Epoch 53 Running loss: 0.005565166048640118\n","Epoch 54 Running loss: 0.008187851014609535\n","Epoch 55 Running loss: 0.006772129013896369\n","Epoch 56 Running loss: 0.027806647288532684\n","Epoch 57 Running loss: 0.007019886955285605\n","Epoch 58 Running loss: 0.007019886955285605\n","Epoch 59 Running loss: 0.007019886955285605\n","Epoch 60 Running loss: 0.007019886955285605\n","Epoch 61 Running loss: 0.007019886955285605\n","Epoch 62 Running loss: 0.007019886955285605\n","Epoch 63 Running loss: 0.007019886955285605\n","Epoch 64 Running loss: 0.007019886955285605\n","Epoch 65 Running loss: 0.007019886955285605\n","Epoch 66 Running loss: 0.007019886955285605\n","Epoch 67 Running loss: 0.013069912267569156\n","Epoch 68 Running loss: 0.007019886955285605\n","Epoch 69 Running loss: 0.007019886955285605\n","Epoch 70 Running loss: 0.007019886955285605\n","Epoch 71 Running loss: 0.007019886955285605\n","Epoch 72 Running loss: 0.007019886955285605\n","Epoch 73 Running loss: 0.007019886955285605\n","Epoch 74 Running loss: 0.007019886955285605\n","Epoch 75 Running loss: 0.007019886955285605\n","Epoch 76 Running loss: 0.007019886955285605\n","Epoch 77 Running loss: 0.007019886955285605\n","Epoch 78 Running loss: 0.007019886955285605\n","Epoch 79 Running loss: 0.007019886955285605\n","Epoch 80 Running loss: 0.007019886955285605\n","Epoch 81 Running loss: 0.007019886955285605\n","Epoch 82 Running loss: 0.007019886955285605\n","Epoch 83 Running loss: 0.007019886955285605\n","Epoch 84 Running loss: 0.007019886955285605\n","Epoch 85 Running loss: 0.00913305766285418\n","Epoch 86 Running loss: 0.007019886955285605\n","Epoch 87 Running loss: 0.007019886955285605\n","Epoch 88 Running loss: 0.007019886955285605\n","Epoch 89 Running loss: 0.007019886955285605\n","Epoch 90 Running loss: 0.007019886955285605\n","Epoch 91 Running loss: 0.010283108717336441\n","Epoch 92 Running loss: 0.007019886955285605\n","Epoch 93 Running loss: 0.007019886955285605\n","Epoch 94 Running loss: 0.008569017385903257\n","Epoch 95 Running loss: 0.007019886955285605\n","Epoch 96 Running loss: 0.007019886955285605\n","Epoch 97 Running loss: 0.007019886955285605\n","Epoch 98 Running loss: 0.007019886955285605\n","Epoch 99 Running loss: 0.007019886955285605\n","Epoch 100 Running loss: 0.007019886955285605\n"]}],"source":["for beta in BETAS:\n","    input_dir = f\"{FILEPATH}/IST non-JNB results/input_encoding/{ENCODING}\"\n","    output_dir = f\"{FILEPATH}/{TEST_TYPE}/MLP/{beta}\"\n","    if  os.path.isfile(f\"{input_dir}/train.pt\") and not os.path.isfile(f\"{output_dir}/train.csv\"):\n","        print(f\"{beta}\")\n","        train_dataset = torch.load(f\"{input_dir}/train.pt\")\n","        test_dataset = torch.load(f\"{input_dir}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = Triangle_Network(features_shape, output_shape, beta=0.9, time_dependent=True).to(device)\n","        num_epochs = 100\n","\n","        criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        batch_size = 120\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","        train_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                # inputs in form of (time, batch, features)\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(0)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        os.makedirs(f\"{output_dir}\", exist_ok=True)\n","        test_results.to_csv(f\"{output_dir}/test.csv\")\n","        train_results.to_csv(f\"{output_dir}/train.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Convolutional Neural Network"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, num_outputs, beta):\n","        super().__init__()\n","\n","        # Initialize layers\n","        self.conv1 = nn.Conv1d(1, 12, kernel_size=5)\n","        self.lif1 = snn.Leaky(beta=beta)\n","        self.conv2 = nn.Conv1d(12, 64, kernel_size=5)\n","        self.lif2 = snn.Leaky(beta=beta)\n","        self.fc1 = nn.Linear(64*29, num_outputs)\n","        self.lif3 = snn.Leaky(beta=beta)\n","\n","    def forward(self, x):\n","\n","        # Initialize hidden states and outputs at t=0\n","        mem1 = self.lif1.init_leaky()\n","        mem2 = self.lif2.init_leaky()\n","        mem3 = self.lif3.init_leaky()\n","\n","        spk_rec = []; mem_rec = []\n","\n","        # Add Channel so in form (time, batch, channel, features)\n","        x = x.unsqueeze(2)\n","\n","        for step in range(x.size(0)):\n","            cur1 = F.max_pool1d(self.conv1(x[step]), 2)\n","            spk1, mem1 = self.lif1(cur1, mem1)\n","\n","            cur2 = F.max_pool1d(self.conv2(spk1), 2)\n","            spk2, mem2 = self.lif2(cur2, mem2)\n","\n","            cur3 = self.fc1(spk2.view(spk2.size(0), -1))\n","\n","            spk3, mem3 = self.lif3(cur3, mem3)\n","\n","            spk_rec.append(spk3)\n","            mem_rec.append(mem3)\n","\n","\n","        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["BETAS = [0.70, 0.80, 0.90, 0.95]"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60e2464c42f4474796e3027c60571410","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.24733674259612354\n","Epoch 2 Running loss: 1.5420367452092827\n","Epoch 3 Running loss: 1.9102369314565446\n","Epoch 4 Running loss: 1.424560668750312\n","Epoch 5 Running loss: 0.6656954951179676\n","Epoch 6 Running loss: 0.5832721356766674\n","Epoch 7 Running loss: 0.22316181583526418\n","Epoch 8 Running loss: 0.19237340372591355\n","Epoch 9 Running loss: 0.10355620137953503\n","Epoch 10 Running loss: 0.05796662534768589\n","Epoch 11 Running loss: 0.014625626631057301\n","Epoch 12 Running loss: 0.007219053876285735\n","Epoch 13 Running loss: 0.007272103914437583\n","Epoch 14 Running loss: 0.007873124207932347\n","Epoch 15 Running loss: 0.006569192491876432\n","Epoch 16 Running loss: 0.007637550274784953\n","Epoch 17 Running loss: 0.008067202263365918\n","Epoch 18 Running loss: 0.0072650614257056875\n","Epoch 19 Running loss: 0.007211300583121876\n","Epoch 20 Running loss: 0.01083904504776001\n","Epoch 21 Running loss: 0.006122776494620326\n","Epoch 22 Running loss: 0.009250653056671825\n","Epoch 23 Running loss: 0.007088837151329358\n","Epoch 24 Running loss: 0.007482097171746885\n","Epoch 25 Running loss: 0.007864689579406104\n","Epoch 26 Running loss: 0.011036764699430131\n","Epoch 27 Running loss: 0.009034597073881009\n","Epoch 28 Running loss: 0.006965706333184775\n","Epoch 29 Running loss: 0.0070327611776967395\n","Epoch 30 Running loss: 0.007824425499279279\n","Epoch 31 Running loss: 0.0074336339538089765\n","Epoch 32 Running loss: 0.008533182461707357\n","Epoch 33 Running loss: 0.008524080434927164\n","Epoch 34 Running loss: 0.00785960861669181\n","Epoch 35 Running loss: 0.010331648988083909\n","Epoch 36 Running loss: 0.007210182496153128\n","Epoch 37 Running loss: 0.007129778305943401\n","Epoch 38 Running loss: 0.00887770089097678\n","Epoch 39 Running loss: 0.006191852184149404\n","Epoch 40 Running loss: 0.004979985892570342\n","Epoch 41 Running loss: 0.008223643413366982\n","Epoch 42 Running loss: 0.011942913547491494\n","Epoch 43 Running loss: 0.008708464642302297\n","Epoch 44 Running loss: 0.015080466628455507\n","Epoch 45 Running loss: 0.0082054989406476\n","Epoch 46 Running loss: 0.007169820820561613\n","Epoch 47 Running loss: 0.006335996972105374\n","Epoch 48 Running loss: 0.01144482476261858\n","Epoch 49 Running loss: 0.010015601548143089\n","Epoch 50 Running loss: 0.013089629979179309\n","Epoch 51 Running loss: 0.007612961359298267\n","Epoch 52 Running loss: 0.007326650352904591\n","Epoch 53 Running loss: 0.007406981798787468\n","Epoch 54 Running loss: 0.008608464806224592\n","Epoch 55 Running loss: 0.006094291282538027\n","Epoch 56 Running loss: 0.006729834186383329\n","Epoch 57 Running loss: 0.007548359445870494\n","Epoch 58 Running loss: 0.006566293930378966\n","Epoch 59 Running loss: 0.006631904495909762\n","Epoch 60 Running loss: 0.007360418002826337\n","Epoch 61 Running loss: 0.0063000067163961\n","Epoch 62 Running loss: 0.0063791319775505185\n","Epoch 63 Running loss: 0.006138405338236199\n","Epoch 64 Running loss: 0.006107432223237551\n","Epoch 65 Running loss: 0.005916517620650343\n","Epoch 66 Running loss: 0.008410080362813542\n","Epoch 67 Running loss: 0.008667730675718655\n","Epoch 68 Running loss: 0.011288044170830578\n","Epoch 69 Running loss: 0.00785008920267367\n","Epoch 70 Running loss: 0.007758873720138598\n","Epoch 71 Running loss: 0.006784999142058741\n","Epoch 72 Running loss: 0.006200016877902582\n","Epoch 73 Running loss: 0.0064889680082424765\n","Epoch 74 Running loss: 0.006792410357167927\n","Epoch 75 Running loss: 0.007052333686298456\n","Epoch 76 Running loss: 0.007612265146578463\n","Epoch 77 Running loss: 0.010860258397964624\n","Epoch 78 Running loss: 0.005630260327467903\n","Epoch 79 Running loss: 0.007508747874738309\n","Epoch 80 Running loss: 0.0057364756646104895\n","Epoch 81 Running loss: 0.0070214251312204065\n","Epoch 82 Running loss: 0.007149742434200006\n","Epoch 83 Running loss: 0.0070663877188588105\n","Epoch 84 Running loss: 0.005599487322968797\n","Epoch 85 Running loss: 0.005501876121011977\n","Epoch 86 Running loss: 0.005570701364362345\n","Epoch 87 Running loss: 0.00721745788098905\n","Epoch 88 Running loss: 0.007712900829010497\n","Epoch 89 Running loss: 0.007029633838147782\n","Epoch 90 Running loss: 0.007844748397985586\n","Epoch 91 Running loss: 0.00703267662669904\n","Epoch 92 Running loss: 0.005106537437703187\n","Epoch 93 Running loss: 0.008014895093326752\n","Epoch 94 Running loss: 0.007018171560269194\n","Epoch 95 Running loss: 0.007823674252238897\n","Epoch 96 Running loss: 0.006383284688376771\n","Epoch 97 Running loss: 0.005996660635875056\n","Epoch 98 Running loss: 0.008334670584803573\n","Epoch 99 Running loss: 0.010236353158189085\n","Epoch 100 Running loss: 0.006773608942001391\n","0.8\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3316c9f6df6144958c129db003f7a2a7","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.34277926618679644\n","Epoch 2 Running loss: 2.980957080000125\n","Epoch 3 Running loss: 3.2053521494515027\n","Epoch 4 Running loss: 1.9626531143919728\n","Epoch 5 Running loss: 1.7190011606429711\n","Epoch 6 Running loss: 0.7754496004634772\n","Epoch 7 Running loss: 0.26732988784107536\n","Epoch 8 Running loss: 0.11004952767405647\n","Epoch 9 Running loss: 0.10460596419751834\n","Epoch 10 Running loss: 0.05826011785683921\n","Epoch 11 Running loss: 0.020460384341474538\n","Epoch 12 Running loss: 0.007058066110641431\n","Epoch 13 Running loss: 0.007016372947266308\n","Epoch 14 Running loss: 0.00702442319248431\n","Epoch 15 Running loss: 0.007156599253511277\n","Epoch 16 Running loss: 0.0070222130598732455\n","Epoch 17 Running loss: 0.007021049912364338\n","Epoch 18 Running loss: 0.007019886955285605\n","Epoch 19 Running loss: 0.007481081417193428\n","Epoch 20 Running loss: 0.01625626879378249\n","Epoch 21 Running loss: 0.007796243928111018\n","Epoch 22 Running loss: 0.007029190992775817\n","Epoch 23 Running loss: 0.007019886955285605\n","Epoch 24 Running loss: 0.009724899412344059\n","Epoch 25 Running loss: 0.007454834807033356\n","Epoch 26 Running loss: 0.006991753182091271\n","Epoch 27 Running loss: 0.005698231712412149\n","Epoch 28 Running loss: 0.007792612400869973\n","Epoch 29 Running loss: 0.008237556527597835\n","Epoch 30 Running loss: 0.01264411515702074\n","Epoch 31 Running loss: 0.00947285574465133\n","Epoch 32 Running loss: 0.009737205962403513\n","Epoch 33 Running loss: 0.007417647983319463\n","Epoch 34 Running loss: 0.009206062307753882\n","Epoch 35 Running loss: 0.005771352460209173\n","Epoch 36 Running loss: 0.010103536870913764\n","Epoch 37 Running loss: 0.007772894427418328\n","Epoch 38 Running loss: 0.011016711068991275\n","Epoch 39 Running loss: 0.007011992291520578\n","Epoch 40 Running loss: 0.007020852436272862\n","Epoch 41 Running loss: 0.007917223266138437\n","Epoch 42 Running loss: 0.008929189972984143\n","Epoch 43 Running loss: 0.0066327522821224534\n","Epoch 44 Running loss: 0.00921003734722686\n","Epoch 45 Running loss: 0.006231632666846815\n","Epoch 46 Running loss: 0.01127939940260622\n","Epoch 47 Running loss: 0.0060619233896176275\n","Epoch 48 Running loss: 0.008407156497906573\n","Epoch 49 Running loss: 0.008248105597572206\n","Epoch 50 Running loss: 0.00870516353521865\n","Epoch 51 Running loss: 0.007757252207198463\n","Epoch 52 Running loss: 0.00854878298962078\n","Epoch 53 Running loss: 0.009290352796975035\n","Epoch 54 Running loss: 0.005751906183009711\n","Epoch 55 Running loss: 0.007221893571055354\n","Epoch 56 Running loss: 0.007597581647074641\n","Epoch 57 Running loss: 0.006842723764931432\n","Epoch 58 Running loss: 0.006768094226955987\n","Epoch 59 Running loss: 0.006932802188891573\n","Epoch 60 Running loss: 0.007562370155566035\n","Epoch 61 Running loss: 0.007210166119158077\n","Epoch 62 Running loss: 0.010588051792912589\n","Epoch 63 Running loss: 0.007260711143572871\n","Epoch 64 Running loss: 0.0058472138909867015\n","Epoch 65 Running loss: 0.00925083796437175\n","Epoch 66 Running loss: 0.006200941273579582\n","Epoch 67 Running loss: 0.007024077942577033\n","Epoch 68 Running loss: 0.007456153178938662\n","Epoch 69 Running loss: 0.008616607314862383\n","Epoch 70 Running loss: 0.007165724286636986\n","Epoch 71 Running loss: 0.007876763328576621\n","Epoch 72 Running loss: 0.010376062065648575\n","Epoch 73 Running loss: 0.008406411630277055\n","Epoch 74 Running loss: 0.008476089555234574\n","Epoch 75 Running loss: 0.0061189594169775135\n","Epoch 76 Running loss: 0.006184765896477257\n","Epoch 77 Running loss: 0.010150785644214374\n","Epoch 78 Running loss: 0.007876830931288747\n","Epoch 79 Running loss: 0.006899753508095543\n","Epoch 80 Running loss: 0.006850894647665298\n","Epoch 81 Running loss: 0.00760037411516086\n","Epoch 82 Running loss: 0.006269321928675563\n","Epoch 83 Running loss: 0.008181456940623518\n","Epoch 84 Running loss: 0.007271356952076141\n","Epoch 85 Running loss: 0.0065066804901098675\n","Epoch 86 Running loss: 0.006369294449925042\n","Epoch 87 Running loss: 0.007066059798097458\n","Epoch 88 Running loss: 0.008801033416876016\n","Epoch 89 Running loss: 0.006637096214599122\n","Epoch 90 Running loss: 0.008193941733326774\n","Epoch 91 Running loss: 0.008204845384286997\n","Epoch 92 Running loss: 0.01021566710913905\n","Epoch 93 Running loss: 0.006295481785989036\n","Epoch 94 Running loss: 0.008266321933878877\n","Epoch 95 Running loss: 0.007730024405561697\n","Epoch 96 Running loss: 0.005568768876714828\n","Epoch 97 Running loss: 0.0053030022274191\n","Epoch 98 Running loss: 0.006603682955233053\n","Epoch 99 Running loss: 0.0067982965955338155\n","Epoch 100 Running loss: 0.009709507131728882\n","0.9\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a67763945bd4c91b77487490f2d96a3","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 0.3468937188291702\n","Epoch 2 Running loss: 1.2815930790032823\n","Epoch 3 Running loss: 3.986843755070013\n","Epoch 4 Running loss: 1.7586728056398824\n","Epoch 5 Running loss: 0.9654735284872329\n","Epoch 6 Running loss: 0.4931799184780913\n","Epoch 7 Running loss: 0.49185715696682186\n","Epoch 8 Running loss: 0.20082880818424895\n","Epoch 9 Running loss: 0.06852072001265261\n","Epoch 10 Running loss: 0.024522717768391863\n","Epoch 11 Running loss: 0.007349246987900414\n","Epoch 12 Running loss: 0.0070304354539694495\n","Epoch 13 Running loss: 0.007021049912364338\n","Epoch 14 Running loss: 0.007023779348062631\n","Epoch 15 Running loss: 0.006490296401535741\n","Epoch 16 Running loss: 0.018569421463500196\n","Epoch 17 Running loss: 0.00844103840593332\n","Epoch 18 Running loss: 0.0068009463838114145\n","Epoch 19 Running loss: 0.015051659303732192\n","Epoch 20 Running loss: 0.0072290535552052265\n","Epoch 21 Running loss: 0.007046196978693953\n","Epoch 22 Running loss: 0.007029629172608494\n","Epoch 23 Running loss: 0.007036786109875567\n","Epoch 24 Running loss: 0.008419600157692029\n","Epoch 25 Running loss: 0.008231534744603947\n","Epoch 26 Running loss: 0.007105503409815292\n","Epoch 27 Running loss: 0.007093108881014986\n","Epoch 28 Running loss: 0.014073871956846585\n","Epoch 29 Running loss: 0.008398431939438889\n","Epoch 30 Running loss: 0.009954965723970066\n","Epoch 31 Running loss: 0.007478635151165362\n","Epoch 32 Running loss: 0.006223862496808695\n","Epoch 33 Running loss: 0.00788724650971044\n","Epoch 34 Running loss: 0.005919198068186117\n","Epoch 35 Running loss: 0.012737213232265874\n","Epoch 36 Running loss: 0.007296269694075417\n","Epoch 37 Running loss: 0.006898287195748034\n","Epoch 38 Running loss: 0.006695900017175431\n","Epoch 39 Running loss: 0.006645171644207769\n","Epoch 40 Running loss: 0.006626619984166691\n","Epoch 41 Running loss: 0.008708755810039874\n","Epoch 42 Running loss: 0.0105096057961924\n","Epoch 43 Running loss: 0.009989999544125395\n","Epoch 44 Running loss: 0.006597805327881639\n","Epoch 45 Running loss: 0.007748948499417534\n","Epoch 46 Running loss: 0.006873294853935607\n","Epoch 47 Running loss: 0.009501117486923267\n","Epoch 48 Running loss: 0.008508591606213261\n","Epoch 49 Running loss: 0.006274387288017395\n","Epoch 50 Running loss: 0.006816634354880824\n","Epoch 51 Running loss: 0.008186545615759901\n","Epoch 52 Running loss: 0.011213305659187487\n","Epoch 53 Running loss: 0.008440059213973462\n","Epoch 54 Running loss: 0.009092315317342838\n","Epoch 55 Running loss: 0.009083487069644867\n","Epoch 56 Running loss: 0.009316282912184256\n","Epoch 57 Running loss: 0.00690679294994464\n","Epoch 58 Running loss: 0.007493891464635587\n","Epoch 59 Running loss: 0.00862263681027836\n","Epoch 60 Running loss: 0.010929887096721906\n","Epoch 61 Running loss: 0.00936352844817189\n","Epoch 62 Running loss: 0.009293890799196384\n","Epoch 63 Running loss: 0.006811881979433492\n","Epoch 64 Running loss: 0.00806583840245256\n","Epoch 65 Running loss: 0.00931300884618546\n","Epoch 66 Running loss: 0.00792856681080291\n","Epoch 67 Running loss: 0.0083693738181751\n","Epoch 68 Running loss: 0.0066502827424972585\n","Epoch 69 Running loss: 0.005224445566963464\n","Epoch 70 Running loss: 0.008417713946808642\n","Epoch 71 Running loss: 0.007267499598451316\n","Epoch 72 Running loss: 0.007242809469326617\n","Epoch 73 Running loss: 0.0076075487624342065\n","Epoch 74 Running loss: 0.010290672603887491\n","Epoch 75 Running loss: 0.00659789140232074\n","Epoch 76 Running loss: 0.0077125323466218695\n","Epoch 77 Running loss: 0.007313708146920981\n","Epoch 78 Running loss: 0.007090934358846646\n","Epoch 79 Running loss: 0.012368576976057059\n","Epoch 80 Running loss: 0.011957259985585562\n","Epoch 81 Running loss: 0.008588716816216611\n","Epoch 82 Running loss: 0.007291266807732871\n","Epoch 83 Running loss: 0.006751520565142647\n","Epoch 84 Running loss: 0.009575393253241103\n","Epoch 85 Running loss: 0.008080397931912457\n","Epoch 86 Running loss: 0.007267358394476553\n","Epoch 87 Running loss: 0.0076980739355849\n","Epoch 88 Running loss: 0.007139425879469314\n","Epoch 89 Running loss: 0.008878177347274634\n","Epoch 90 Running loss: 0.006707682086827275\n","Epoch 91 Running loss: 0.0065247919678259585\n","Epoch 92 Running loss: 0.00857993665213783\n","Epoch 93 Running loss: 0.006441121855482888\n","Epoch 94 Running loss: 0.007276122657635722\n","Epoch 95 Running loss: 0.008057764734323033\n","Epoch 96 Running loss: 0.008132220838016596\n","Epoch 97 Running loss: 0.005341900411410073\n","Epoch 98 Running loss: 0.009104815344460095\n","Epoch 99 Running loss: 0.006823305314341293\n","Epoch 100 Running loss: 0.007490957506929343\n","0.95\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d630127722644920b1cdca0af6725275","version_major":2,"version_minor":0},"text/plain":["Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 Running loss: 1.04791473047421\n","Epoch 2 Running loss: 2.7375246480630993\n","Epoch 3 Running loss: 0.8326415528123752\n","Epoch 4 Running loss: 0.3778421139945618\n","Epoch 5 Running loss: 0.23114359188384523\n","Epoch 6 Running loss: 0.11908628993902724\n","Epoch 7 Running loss: 0.13817217098638274\n","Epoch 8 Running loss: 0.07654969608440948\n","Epoch 9 Running loss: 0.03132152366942872\n","Epoch 10 Running loss: 0.008487506415516424\n","Epoch 11 Running loss: 0.00699471360959184\n","Epoch 12 Running loss: 0.007019886955285605\n","Epoch 13 Running loss: 0.00701671991104516\n","Epoch 14 Running loss: 0.007100851962360711\n","Epoch 15 Running loss: 0.007396579169617674\n","Epoch 16 Running loss: 0.005829845564053081\n","Epoch 17 Running loss: 0.01074844122694704\n","Epoch 18 Running loss: 0.007526621079673401\n","Epoch 19 Running loss: 0.0071566527643904514\n","Epoch 20 Running loss: 0.006395100023799811\n","Epoch 21 Running loss: 0.007090931311963846\n","Epoch 22 Running loss: 0.023884114365989025\n","Epoch 23 Running loss: 0.010919838858107789\n","Epoch 24 Running loss: 0.007855893895268059\n","Epoch 25 Running loss: 0.01114878315514269\n","Epoch 26 Running loss: 0.008601835550972449\n","Epoch 27 Running loss: 0.007227274937370715\n","Epoch 28 Running loss: 0.02688232264198815\n","Epoch 29 Running loss: 0.00796356140234219\n","Epoch 30 Running loss: 0.006812934379521817\n","Epoch 31 Running loss: 0.0070523568473684906\n","Epoch 32 Running loss: 0.006794224151026327\n","Epoch 33 Running loss: 0.007789682085140826\n","Epoch 34 Running loss: 0.007898092650757811\n","Epoch 35 Running loss: 0.007489179460385356\n","Epoch 36 Running loss: 0.008114329161354529\n","Epoch 37 Running loss: 0.009281320503344551\n","Epoch 38 Running loss: 0.008519549529773358\n","Epoch 39 Running loss: 0.006910099579503361\n","Epoch 40 Running loss: 0.010753524570038524\n","Epoch 41 Running loss: 0.00821166088025029\n","Epoch 42 Running loss: 0.008361007078006244\n","Epoch 43 Running loss: 0.008737398031801461\n","Epoch 44 Running loss: 0.005768002376864893\n","Epoch 45 Running loss: 0.007891970891922047\n","Epoch 46 Running loss: 0.006644451580108545\n","Epoch 47 Running loss: 0.008386272020614186\n","Epoch 48 Running loss: 0.011190099076341135\n","Epoch 49 Running loss: 0.0061772135309517955\n","Epoch 50 Running loss: 0.005393490302391326\n","Epoch 51 Running loss: 0.008885421882422206\n","Epoch 52 Running loss: 0.005896549340518461\n","Epoch 53 Running loss: 0.006979203452698339\n","Epoch 54 Running loss: 0.006013913657337713\n","Epoch 55 Running loss: 0.008891747782405573\n","Epoch 56 Running loss: 0.0064170643353995424\n","Epoch 57 Running loss: 0.009355482365638493\n","Epoch 58 Running loss: 0.009898898319695324\n","Epoch 59 Running loss: 0.00646560689130911\n","Epoch 60 Running loss: 0.006740699846523639\n","Epoch 61 Running loss: 0.009627850863118521\n","Epoch 62 Running loss: 0.0076426231442168115\n","Epoch 63 Running loss: 0.006905615710602782\n","Epoch 64 Running loss: 0.008517742156982422\n","Epoch 65 Running loss: 0.008136375453144598\n","Epoch 66 Running loss: 0.00721138163496511\n","Epoch 67 Running loss: 0.006868917911578291\n","Epoch 68 Running loss: 0.006972519734416145\n","Epoch 69 Running loss: 0.015809393348023535\n","Epoch 70 Running loss: 0.007747154456738846\n","Epoch 71 Running loss: 0.010110099426509377\n","Epoch 72 Running loss: 0.007062629281808012\n","Epoch 73 Running loss: 0.007842521317088947\n","Epoch 74 Running loss: 0.011129135521836937\n","Epoch 75 Running loss: 0.007103195015233927\n","Epoch 76 Running loss: 0.005796611594696776\n","Epoch 77 Running loss: 0.0073204295703778255\n","Epoch 78 Running loss: 0.006517289450374274\n","Epoch 79 Running loss: 0.009006345995698875\n","Epoch 80 Running loss: 0.004872893777708657\n","Epoch 81 Running loss: 0.005216063068697627\n","Epoch 82 Running loss: 0.005590688877593214\n","Epoch 83 Running loss: 0.0057420546825701435\n","Epoch 84 Running loss: 0.005461203273774062\n","Epoch 85 Running loss: 0.009259587469192358\n","Epoch 86 Running loss: 0.00646885801047182\n","Epoch 87 Running loss: 0.00644082902148128\n","Epoch 88 Running loss: 0.007952280890065642\n","Epoch 89 Running loss: 0.005978482028546806\n","Epoch 90 Running loss: 0.006367806809397932\n","Epoch 91 Running loss: 0.008392774258939603\n","Epoch 92 Running loss: 0.009020214358838602\n","Epoch 93 Running loss: 0.006143865858415921\n","Epoch 94 Running loss: 0.00657993032576177\n","Epoch 95 Running loss: 0.007075942838534761\n","Epoch 96 Running loss: 0.006173940512318962\n","Epoch 97 Running loss: 0.00582283487716041\n","Epoch 98 Running loss: 0.006741574504219305\n","Epoch 99 Running loss: 0.005740957733350821\n","Epoch 100 Running loss: 0.004538609308544725\n"]}],"source":["for beta in BETAS:\n","    input_dir = f\"{FILEPATH}/IST non-JNB results/input_encoding/{ENCODING}\"\n","    output_dir = f\"{FILEPATH}/{TEST_TYPE}/CNN/{beta}\"\n","    if not os.path.isfile(f\"{output_dir}/train.csv\") and os.path.isfile(f\"{input_dir}/train.pt\"):\n","        print(f\"{beta}\")\n","        train_dataset = torch.load(f\"{input_dir}/train.pt\")\n","        test_dataset = torch.load(f\"{input_dir}/test.pt\")\n","\n","        # Get the shape of the first sample (train_dataset[0]) of data (x) within the dataset\n","        x_shape = train_dataset[0][0].shape\n","\n","        # Assuming the shape is t x f\n","        features_shape = x_shape[1]\n","        POP_ENCODING = 10\n","        classes = len(label_encoder.classes_)\n","        output_shape = classes * POP_ENCODING\n","\n","\n","        model = CNN(output_shape, beta=0.9).to(device)\n","        num_epochs = 100\n","\n","        criterion = PopulationCrossEntropyLoss(num_classes=classes)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        batch_size = 120\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n","\n","        test_results = df()\n","        train_results = df()\n","\n","        epoch_progress_bar = tqdm(total=num_epochs, desc=\"Training Progress\", position=0)\n","\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            total = 0\n","            model.train()\n","            for inputs, targets in train_loader:\n","                # inputs in form of (time, batch, features)\n","                inputs, targets = inputs.transpose(0, 1).to(device), targets.to(device)\n","\n","                spikes, _ = model(inputs)\n","\n","                loss = criterion(spikes, targets)\n","\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                total += spikes.size(0)\n","\n","            epoch_progress_bar.update(1)\n","\n","            print(f\"Epoch {epoch+1} Running loss: {running_loss/total}\")\n","            \n","            # Print average loss for the epoch\n","            if ((epoch+1) % 5 == 0):\n","                test_results = test_spiking_network(model, test_dataset, criterion, test_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'test')\n","                train_results = test_spiking_network(model, train_dataset, criterion, train_results, epoch, device, num_classes = classes, printable = epoch_progress_bar, train_test = 'train')\n","\n","        del model\n","        del inputs\n","        del targets\n","        del optimizer\n","        del criterion\n","        del loss\n","        gc.collect()\n","        if device == 'cuda': torch.cuda.empty_cache()\n","        elif device == 'mps': torch.mps.empty_cache()\n","\n","        os.makedirs(f\"{output_dir}\", exist_ok=True)\n","        test_results.to_csv(f\"{output_dir}/test.csv\")\n","        train_results.to_csv(f\"{output_dir}/train.csv\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["QrnOeGaHh25K","43m8jiuhh25L","st4_dGj4h25N"],"gpuType":"A100","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
