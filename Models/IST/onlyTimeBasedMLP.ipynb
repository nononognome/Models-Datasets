{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import pandas as pd\n",
    "import spectrograms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_train = ...\n",
    "mfcc_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time based input, Leaky MLP-SNN, 1 Hidden Layer (FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease set training and testing set to be specific variable\n",
    "# Ease of changing spectrogram type\n",
    "train, test = mfcc_train, mfcc_test\n",
    "\n",
    "# Set input, step, and hidden node size based on a single training sample (assuming theyre uniform and normalised)\n",
    "sample, _ = train[0]                # Sample in form Time x Frequency Bins\n",
    "\n",
    "num_inputs = sample.shape[1]        # Depends on the input spectrogram (number of frequency bins; y-axis)\n",
    "num_steps = sample.shape[0]         # Number of samples per spectrogram (or spectrogram sample rate * time of audio; x-axis)\n",
    "num_hidden = num_inputs // 2        # Ideally half the number of inputs (originally 1000)\n",
    "num_hidden_layers = 1               # 2 hidden layers with num_hidden nodes each\n",
    "num_ouputs = 2                      # Either Music or Non-Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set decay rate and threshold:\n",
    "# Arbitrary threshold, decay rate set close to 1 for reasonable accuracy -- given delta_t << tau in: beta = (1 - delta_t/tau)\n",
    "beta = 0.95\n",
    "threshold = 0.75\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # Initialise network with 2 forward connections (linear connections) and 2 leaky integrated fire layers (hidden and output)\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_ouputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    # Define a forward pass assuming x is normalised data (i.e. all values in [0,1])\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        spk1_rec = []\n",
    "        mem1_rec = []\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # Step through the time sets within the data -- get current from data at a given time, forward it to lif\n",
    "        # Use the lif spikes to generate a current from spikes, feed this through a second (output) lif\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[:,step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk1_rec.append(spk1)\n",
    "            mem1_rec.append(mem1)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk1_rec, dim=0), torch.stack(mem1_rec, dim=0), torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrarily set num_epochs depending on converging rate\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialise counter, and loss histories\n",
    "counter = 0\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "# Arbitrarily set batch_size -- ideally based on memory utilisation and speed\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise network. Set to cuda where available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper output functions\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    _, _, output, _ = net(data)\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer(epoch, iter_counter, data, targets, test_data, test_targets):\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and test dataloaders based on batch_size\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise loss (CE Loss) and Adam optimiser -- learning rate is a hyperparameter\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "# Train for num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Loop through batches -- separate out data and targets in each batch [batch_size x times x frequencies]\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward Pass in train mode\n",
    "        net.train()\n",
    "        spk1_rec, mem1_rec, spk2_rec, mem2_rec = net(data)\n",
    "\n",
    "        # Sum loss over time (batch membrane2 records for each time step against batch targets)\n",
    "        loss_val = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem2_rec[step], targets)\n",
    "\n",
    "        # Weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Loss history storage\n",
    "        # loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set (for loss history in current form)\n",
    "        # Preformance boost if only evaluated when counter / 50 \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Forward Pass in test ode\n",
    "            test_spk1, test_mem1, test_spk2, test_mem2 = net(test_data)\n",
    "\n",
    "            # Sum loss over time for test set\n",
    "            test_loss = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem2[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer(epoch, iter_counter, data, targets, test_data, test_targets)\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all train and test data\n",
    "data, targets = train.tensors[0], train.tensors[1]\n",
    "test_data, test_targets = test.tensors[0], test.tensors[1]\n",
    "\n",
    "# Find overall train accuracy\n",
    "# Check max spikes in output neurons, compare against targets\n",
    "_, _, output, _ = net(data.to(device))\n",
    "_, idx = output.sum(dim=0).max(1)\n",
    "acc = np.mean((targets.to(device) == idx).detach().cpu().numpy())\n",
    "print(acc)\n",
    "\n",
    "# Find overall test accuracy\n",
    "_, _, output, _ = net(test_data.to(device))\n",
    "_, idx = output.sum(dim=0).max(1)\n",
    "acc = np.mean((test_targets.to(device) == idx).detach().cpu().numpy())\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
